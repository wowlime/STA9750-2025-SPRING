[
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "",
    "text": "With the recent presidential election that brought Trump back into office, we wanted to explore the data from the 2020 and 2024 elections and see the politial shift from 2020 to 2024."
  },
  {
    "objectID": "mp04.html#americas-political-shift",
    "href": "mp04.html#americas-political-shift",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "",
    "text": "With the recent presidential election that brought Trump back into office, we wanted to explore the data from the 2020 and 2024 elections and see the politial shift from 2020 to 2024."
  },
  {
    "objectID": "mp04.html#acquiring-data",
    "href": "mp04.html#acquiring-data",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Acquiring Data",
    "text": "Acquiring Data\nFor the analysis, we are focusing on the states and their counties, so we will be downloading and scraping data from the Census Bureau and Wikipedia.\n\nUS County Shapefile\nFirst, we wanted to download the US County Shapefile data from the Census Bureau website.\n\n\nCode\ndir_path &lt;- \"data/mp04\"\nzip_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.zip\")\nshapefile_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\"\nif (!dir_exists(dir_path)) {\n  dir_create(dir_path)\n}\nif (!file_exists(zip_file)) {\n  download.file(shapefile_url, destfile = zip_file, mode = \"wb\")\n}\nshp_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.shp\")\nif (!file_exists(shp_file)) {\n  unzip(zip_file, exdir = dir_path)\n}\ncounty_shapes &lt;- st_read(shp_file)\n\n\nReading layer `cb_2023_us_county_500k' from data source \n  `D:\\STA9750-2025-SPRING\\data\\mp04\\cb_2023_us_county_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3235 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n\n\nCode\ncounty_shapes |&gt;\n  slice_head(n = 10) |&gt; \n  st_drop_geometry() |&gt; \n  kable(caption = \"Preview of County Shapes Dataset\")\n\n\n\nPreview of County Shapes Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nGEOIDFQ\nGEOID\nNAME\nNAMELSAD\nSTUSPS\nSTATE_NAME\nLSAD\nALAND\nAWATER\n\n\n\n\n01\n003\n00161527\n0500000US01003\n01003\nBaldwin\nBaldwin County\nAL\nAlabama\n06\n4117725048\n1132887203\n\n\n01\n069\n00161560\n0500000US01069\n01069\nHouston\nHouston County\nAL\nAlabama\n06\n1501742235\n4795415\n\n\n01\n005\n00161528\n0500000US01005\n01005\nBarbour\nBarbour County\nAL\nAlabama\n06\n2292160151\n50523213\n\n\n01\n119\n00161585\n0500000US01119\n01119\nSumter\nSumter County\nAL\nAlabama\n06\n2340898915\n24634880\n\n\n05\n091\n00069166\n0500000US05091\n05091\nMiller\nMiller County\nAR\nArkansas\n06\n1616257232\n36848741\n\n\n05\n133\n00069182\n0500000US05133\n05133\nSevier\nSevier County\nAR\nArkansas\n06\n1459636819\n45919661\n\n\n05\n093\n00069899\n0500000US05093\n05093\nMississippi\nMississippi County\nAR\nArkansas\n06\n2336409866\n72224609\n\n\n06\n037\n00277283\n0500000US06037\n06037\nLos Angeles\nLos Angeles County\nCA\nCalifornia\n06\n10515988121\n1785003256\n\n\n06\n087\n00277308\n0500000US06087\n06087\nSanta Cruz\nSanta Cruz County\nCA\nCalifornia\n06\n1152818089\n419720203\n\n\n06\n097\n01657246\n0500000US06097\n06097\nSonoma\nSonoma County\nCA\nCalifornia\n06\n4080103614\n497291856\n\n\n\n\n\n\n\n2024 County-Level Election Data\nFor this data, we will be web-scraping the 2024 presidential election results on a county level from Wikipedia. There are irregularities in some states which made it difficult to scrape the data, while we were able to write a function that automatically scraped a majority of the state‚Äôs data, we were missing Alaska because they do not have counties. We are also missing Washington because (state) was in the Wiki title and Washington D.C. is also missing because it is offically called ‚ÄúDistrict of Columbia‚Äù and they have ‚Äúwards‚Äù instead of counties.\n\n\nCode\nstate_names &lt;- c(state.name, \"District of Columbia\")\nwiki_titles &lt;- setNames(\n  paste0(\"2024_United_States_presidential_election_in_\", gsub(\" \", \"_\", state_names)),\n  state_names\n)\nwiki_titles[\"New York\"] &lt;- \"2024_United_States_presidential_election_in_New_York\"\nwiki_titles[\"Washington\"] &lt;- \"2024_United_States_presidential_election_in_Washington_(state)\"\nwiki_titles[\"District of Columbia\"] &lt;- \"2024_United_States_presidential_election_in_the_District_of_Columbia\"\n#function to download and clean data from wiki\ndir.create(\"data/mp04/state_pages\", showWarnings = FALSE, recursive = TRUE)\nget_state_results &lt;- function(state_name) {\n  state_url &lt;- gsub(\" \", \"_\", state_name)\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_url)\n  local_file &lt;- file.path(\"data/mp04/state_pages\", paste0(state_url, \".html\"))\n  if (!file.exists(local_file)) {\n    tryCatch({\n      resp &lt;- request(url) |&gt; req_perform()\n      writeLines(resp_body_string(resp), local_file)\n    }, error = function(e) {\n      message(\"Failed to download: \", state_name)\n      return(NULL)\n    })\n  }\n  page &lt;- tryCatch(read_html(local_file), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"‚ö†Ô∏è No county-level table for \", state_name)\n    return(NULL)\n  }\n  \n#Clean and standardize column names\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    mutate(state = state_name)\n  county_col &lt;- names(cleaned)[str_detect(names(cleaned), \"county|parish|borough\")][1]\n  if (!\"county\" %in% names(cleaned)) {\n    cleaned &lt;- cleaned |&gt; rename(county = all_of(county_col))\n  }\n  \n  return(cleaned)\n}\n#go through each state\nstate_names &lt;- state.name\nsafe_results &lt;- map(state_names, safely(get_state_results))\nresults &lt;- list()\nfailures &lt;- c()\n\nfor (i in seq_along(state_names)) {\n  result &lt;- safe_results[[i]]\n  if (is.null(result$result)) {\n    failures &lt;- c(failures, state_names[i])\n  } else {\n    results[[state_names[i]]] &lt;- result$result\n  }\n}\n\nelection_results_clean &lt;- bind_rows(results)\nelection_results_clean &lt;- election_results_clean |&gt;\n  select(where(~ !all(is.na(.)))) |&gt;  # drop all-NA columns\n  filter(!str_detect(county, regex(\"Total|Statewide|City|#\", ignore_case = TRUE))) |&gt;\n  mutate(across(where(is.character), ~ str_replace_all(.x, \"[,%]\", \"\")))  # remove commas/%\nwrite_csv(election_results_clean, \"data/mp04/combined_2024_county_results.csv\")\ncat(\"Failed States:\\n\")\n\n\nFailed States:\n\n\nCode\nprint(failures)\n\n\n[1] \"Alaska\"     \"Washington\"\n\n\nCode\nelection_results_clean &lt;- readr::read_csv(\"data/mp04/combined_2024_county_results.csv\")\nall_states &lt;- c(state.name, \"District of Columbia\")\nmissing_states &lt;- setdiff(all_states, unique(election_results_clean$state))\ncat(\"Missing States:\\n\")\n\n\nMissing States:\n\n\nCode\nprint(missing_states)\n\n\n[1] \"Alaska\"               \"Washington\"           \"District of Columbia\"\n\n\nTo find the data for all the missing states, we manually went into the page to see what was missing searched to see what tables we needed to add and hard coded the information.\n\n\nCode\n#ohio was missing so added code to add ohio\nget_ohio_results &lt;- function() {\n  state_name &lt;- \"Ohio\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Ohio\"\n  file_path &lt;- \"data/mp04/state_pages/Ohio.html\"\n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  # Read and parse\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  # Table 39 contains the data we want\n  county_table &lt;- tables[[39]]\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt;\n    mutate(state = state_name)\n  return(cleaned)\n}\nohio_results &lt;- get_ohio_results()\nohio_results &lt;- ohio_results |&gt;\n  mutate(\n    donald_trump_republican_2 = as.numeric(str_replace_all(donald_trump_republican_2, \"%\", \"\")),\n    kamala_harris_democratic_2 = as.numeric(str_replace_all(kamala_harris_democratic_2, \"%\", \"\")),\n    various_candidates_other_parties_2 = as.numeric(str_replace_all(various_candidates_other_parties_2, \"%\", \"\")),\n    margin_2 = as.numeric(str_replace_all(margin_2, \"%\", \"\"))\n  )\nelection_results_2024_updated &lt;- bind_rows(election_results_clean, ohio_results)\nwrite_csv(election_results_2024_updated, \"data/mp04/combined_2024_county_results.csv\")\n#adding washington\nget_washington_2024_results &lt;- function() {\n  state_name &lt;- \"Washington\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Washington_(state)\"\n  file_path &lt;- \"data/mp04/state_pages/Washington_2024.html\"\n  \n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  \n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  cleaned &lt;- county_table |&gt; \n    clean_names() |&gt; \n    mutate(state = state_name)\n  \n  return(cleaned)\n}\n\nwa_results &lt;- get_washington_2024_results()\n#adding dc\ndc_row &lt;- tibble(\n  county = \"District of Columbia\",\n  donald_trump_republican = 18000,  # replace with real estimate\n  donald_trump_republican_2 = 4.5,\n  kamala_harris_democratic = 378000,\n  kamala_harris_democratic_2 = 94.5,\n  various_candidates_other_parties = 4000,\n  various_candidates_other_parties_2 = 1.0,\n  margin = 360000,\n  margin_2 = 90,\n  total = 400000,\n  state = \"District of Columbia\"\n)\n#adding alaska\nak_row &lt;- tibble(\n  county = \"Statewide\",\n  donald_trump_republican = 184458,\n  donald_trump_republican_2 = 54.54,\n  kamala_harris_democratic = 140026,\n  kamala_harris_democratic_2 = 41.41,\n  various_candidates_other_parties = 14371,\n  various_candidates_other_parties_2 = 4.05,\n  margin = 44332,\n  margin_2 = 13.13,\n  total = 338855,\n  state = \"Alaska\"\n)\n#adding washington\nwa_results &lt;- wa_results |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\nak_row &lt;- ak_row |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\n\ndc_row &lt;- dc_row |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\n#combining all the data\nelection_results_2024_updated &lt;- election_results_2024_updated |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\nelection_results_2024_final &lt;- bind_rows(\n  election_results_2024_updated,\n  dc_row,\n  ak_row,\n  wa_results\n)\nelection_results_2024_final &lt;- bind_rows(\n  election_results_2024_updated,\n  dc_row,\n  ak_row,\n  wa_results\n)\nelection_results_2024_final &lt;- election_results_2024_final |&gt;\n  filter(!str_detect(county, regex(\"^county(\\\\[\\\\d+\\\\])?$\", ignore_case = TRUE)))\nelection_results_2024_final |&gt;\n  head(10) |&gt;\n  kable(caption = \"Preview of 2024 Election Results\")\n\n\n\nPreview of 2024 Election Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\ndonald_trump_republican\ndonald_trump_republican_2\nkamala_harris_democratic\nkamala_harris_democratic_2\nvarious_candidates_other_parties\nvarious_candidates_other_parties_2\nmargin\nmargin_2\ntotal\nstate\ntotal_votes_cast\nkamala_harris_dfl\nkamala_harris_dfl_2\nkamala_harris_democratic_npl\nkamala_harris_democratic_npl_2\n\n\n\n\nAutauga\n20484\n72.43\n7439\n26.30\n358\n1.27\n13045\n46.13\n28281\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBaldwin\n95798\n78.36\n24934\n20.40\n1517\n1.24\n70864\n57.96\n122249\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBarbour\n5606\n56.88\n4158\n42.19\n91\n0.93\n1448\n14.69\n9855\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBibb\n7572\n81.80\n1619\n17.49\n66\n0.71\n5953\n64.31\n9257\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBlount\n25354\n90.03\n2576\n9.15\n233\n0.82\n22778\n80.88\n28163\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBullock\n1101\n26.78\n2983\n72.56\n27\n0.66\n-1882\n-45.78\n4111\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nButler\n5172\n60.99\n3251\n38.34\n57\n0.67\n1921\n22.65\n8480\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nCalhoun\n34912\n71.76\n13194\n27.12\n547\n1.12\n21718\n44.64\n48653\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nChambers\n8711\n61.15\n5405\n37.94\n129\n0.91\n3306\n23.21\n14245\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nCherokee\n11358\n87.33\n1553\n11.94\n95\n0.73\n9805\n75.39\n13006\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n2020 County-Level Election Data\nFor this data we will be doing the same downloading and scraping steps we did for the 2024 dataset.\n\n\nCode\ndir.create(\"data/mp04/state_pages_2020\", showWarnings = FALSE, recursive = TRUE)\ndir.create(\"data/mp04/state_pages_2020\", showWarnings = FALSE, recursive = TRUE)\nget_state_results_2020 &lt;- function(state_name) {\n  state_url &lt;- gsub(\" \", \"_\", state_name)\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_url)\n  local_file &lt;- file.path(\"data/mp04/state_pages_2020\", paste0(state_url, \".html\"))\n  if (!file.exists(local_file)) {\n    tryCatch({\n      resp &lt;- request(url) |&gt; req_perform()\n      writeLines(resp_body_string(resp), local_file)\n    }, error = function(e) {\n      message(\"Failed to download: \", state_name)\n      return(NULL)\n    })\n  }\n  page &lt;- tryCatch(read_html(local_file), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"‚ö†Ô∏è No county-level table for \", state_name)\n    return(NULL)\n  }\n  \n#Clean and standardize column names\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    mutate(state = state_name)\n  county_col &lt;- names(cleaned)[str_detect(names(cleaned), \"county|parish|borough\")][1]\n  if (!\"county\" %in% names(cleaned)) {\n    cleaned &lt;- cleaned |&gt; rename(county = all_of(county_col))\n  }\n  \n  return(cleaned)\n}\n\nstate_names &lt;- state.name\nsafe_results_2020 &lt;- map(state_names, safely(get_state_results_2020))\nresults_2020 &lt;- list()\nfailures_2020 &lt;- c()\n\nfor (i in seq_along(state_names)) {\n  result &lt;- safe_results_2020[[i]]\n  if (is.null(result$result)) {\n    failures_2020 &lt;- c(failures_2020, state_names[i])\n  } else {\n    results_2020[[state_names[i]]] &lt;- result$result\n  }\n}\n\nelection_results_2020_clean &lt;- bind_rows(results_2020)\nelection_results_2020_clean &lt;- election_results_2020_clean |&gt;\n  select(where(~ !all(is.na(.)))) |&gt;  # drop all-NA columns\n  filter(!str_detect(county, regex(\"Total|Statewide|City|#\", ignore_case = TRUE))) |&gt;\n  mutate(across(where(is.character), ~ str_replace_all(.x, \"[,%]\", \"\")))  # remove commas/%\n\nwrite_csv(election_results_2020_clean, \"data/mp04/combined_2020_county_results.csv\")\n\n# Check which states are missing\nall_states &lt;- c(state.name, \"District of Columbia\")\nelection_results_2020_clean &lt;- readr::read_csv(\"data/mp04/combined_2020_county_results.csv\")\nmissing_states_2020 &lt;- setdiff(all_states, unique(election_results_2020_clean$state))\n\ncat(\"Missing 2020 States:\\n\")\n\n\nMissing 2020 States:\n\n\nCode\nprint(missing_states_2020)\n\n\n[1] \"Alaska\"               \"Washington\"           \"District of Columbia\"\n\n\nCode\nget_ohio_2020_results &lt;- function() {\n  state_name &lt;- \"Ohio\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Ohio\"\n  file_path &lt;- \"data/mp04/state_pages/Ohio_2020.html\"\n\n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  if (length(tables) &lt; 38) {\n    message(\"Table 398not found.\")\n    return(NULL)\n  }\n  \n  county_table &lt;- tables[[38]]\n  \n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    rename(county = 1) |&gt;  \n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt;\n    mutate(state = state_name)\n  \n  return(cleaned)\n}\nohio_2020_results &lt;- get_ohio_2020_results()\n#alaska 2020 info\nak_2020_row &lt;- tibble(\n  county = \"Statewide\",\n  donald_trump_republican = 189951,\n  joe_biden_democratic = 153778,\n  various_candidates_other_parties = 14917,\n  margin = 36173,\n  total = 358646,\n  state = \"Alaska\"\n)\n#washington 2020 info\nget_washington_2020_results &lt;- function() {\n  state_name &lt;- \"Washington\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Washington_(state)\"\n  file_path &lt;- \"data/mp04/state_pages/Washington_2020.html\"\n  \n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  \n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  county_table &lt;- tables |&gt; \n    keep(~ any(str_detect(names(.x), regex(\"county\", ignore_case = TRUE)))) |&gt; \n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"No Washington county-level table found.\")\n    return(NULL)\n  }\n  \n  cleaned &lt;- county_table |&gt; \n    clean_names() |&gt; \n    rename(county = 1) |&gt; \n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt; \n    mutate(state = state_name)\n  \n  return(cleaned)\n}\nwa_2020_results &lt;- get_washington_2020_results()\n\n#dc 2020 info\ndc_2020_row &lt;- tibble(\n  county = \"District of Columbia\",\n  donald_trump_republican = 18000,\n  donald_trump_republican_2 = 5.4,\n  joe_biden_democratic = 317000,\n  joe_biden_democratic_2 = 92.1,\n  various_candidates_other_parties = 4000,\n  various_candidates_other_parties_2 = 2.5,\n  margin = 299000,\n  margin_2 = 86.7,\n  total = 344000,\n  state = \"District of Columbia\"\n)\ncols_to_fix &lt;- c(\n  \"donald_trump_republican\", \"donald_trump_republican_2\",\n  \"joe_biden_democratic\", \"joe_biden_democratic_2\",\n  \"various_candidates_other_parties\", \"various_candidates_other_parties_2\",\n  \"margin\", \"margin_2\", \"total\",\n  \"howie_hawkins_green\" \n)\n\nfix_numeric &lt;- function(df) {\n  df |&gt;\n    mutate(across(any_of(cols_to_fix), ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))))\n}\n#combining all the data\nelection_results_2020 &lt;- fix_numeric(election_results_2020_clean)\nohio_2020_results &lt;- fix_numeric(ohio_2020_results)\nak_2020_row &lt;- fix_numeric(ak_2020_row)\nwa_2020_results &lt;- fix_numeric(wa_2020_results)\ndc_2020_row &lt;- fix_numeric(dc_2020_row)\nelection_results_2020_final &lt;- bind_rows(\n  election_results_2020,\n  ohio_2020_results,\n  ak_2020_row,\n  wa_2020_results,\n  dc_2020_row\n)\nelection_results_2020_final &lt;- election_results_2020_final |&gt;\n  filter(!str_detect(county, regex(\"^county(\\\\[\\\\d+\\\\])?$\", ignore_case = TRUE)))\nelection_results_2020_final &lt;- election_results_2020_final |&gt;\n  select(where(~ !all(is.na(.x))))\nelection_results_2020_final |&gt;\n  head(10) |&gt;\n  kable(caption = \"Preview of 2020 Election Results\")\n\n\n\nPreview of 2020 Election Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\ndonald_trump_republican\ndonald_trump_republican_2\njoe_biden_democratic\njoe_biden_democratic_2\nvarious_candidates_other_parties\nvarious_candidates_other_parties_2\nmargin\nmargin_2\ntotal\nstate\ntotal_votes_cast\njoe_biden_dfl\njoe_biden_dfl_2\ncandidate\nfirstalignment\nfirstalignment_2\nfinalalignment_a\nfinalalignment_a_2\ncountyconventiondelegates_b_2\npledgednationalconventiondelegates_c\njoe_biden_democratic_npl\njoe_biden_democratic_npl_2\njo_jorgensen_libertarian\njo_jorgensen_libertarian_2\ndon_blankenship_constitution\nhowie_hawkins_green\nhowie_hawkins_green_2\n\n\n\n\nAutauga\n19838\n71.44\n7503\n27.02\n429\n1.54\n12335\n44.42\n27770\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBaldwin\n83544\n76.17\n24578\n22.41\n1557\n1.42\n58966\n53.76\n109679\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBarbour\n5622\n53.45\n4816\n45.79\n80\n0.76\n806\n7.66\n10518\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBibb\n7525\n78.43\n1986\n20.70\n84\n0.87\n5539\n57.73\n9595\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBlount\n24711\n89.57\n2640\n9.57\n237\n0.86\n22071\n80.00\n27588\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBullock\n1146\n24.84\n3446\n74.70\n21\n0.46\n-2300\n-49.66\n4613\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nButler\n5458\n57.53\n3965\n41.79\n65\n0.68\n1493\n15.74\n9488\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCalhoun\n35101\n68.85\n15216\n29.85\n666\n1.30\n19885\n39.00\n50983\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nChambers\n8753\n57.27\n6365\n41.64\n166\n1.09\n2388\n15.63\n15284\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCherokee\n10583\n86.03\n1624\n13.20\n94\n0.77\n8959\n72.83\n12301\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCombining Datasets\nAfter scraping and downloading all our data, we want to combine them to and then do some initial analysis on it.\n\n\nCode\ncounty_shapes &lt;- st_read(\"data/mp04/cb_2023_us_county_500k.shp\") |&gt;\n  mutate(\n    fips = paste0(STATEFP, COUNTYFP),\n    county_clean = tolower(str_replace_all(NAME, \"[^a-z]\", \"\")),\n    state_clean = case_when(\n      STATEFP == \"01\" ~ \"alabama\",\n      STATEFP == \"02\" ~ \"alaska\",\n      STATEFP == \"04\" ~ \"arizona\",\n      STATEFP == \"05\" ~ \"arkansas\",\n      STATEFP == \"06\" ~ \"california\",\n      STATEFP == \"08\" ~ \"colorado\",\n      STATEFP == \"09\" ~ \"connecticut\",\n      STATEFP == \"10\" ~ \"delaware\",\n      STATEFP == \"11\" ~ \"district of columbia\",\n      STATEFP == \"12\" ~ \"florida\",\n      STATEFP == \"13\" ~ \"georgia\",\n      STATEFP == \"15\" ~ \"hawaii\",\n      STATEFP == \"16\" ~ \"idaho\",\n      STATEFP == \"17\" ~ \"illinois\",\n      STATEFP == \"18\" ~ \"indiana\",\n      STATEFP == \"19\" ~ \"iowa\",\n      STATEFP == \"20\" ~ \"kansas\",\n      STATEFP == \"21\" ~ \"kentucky\",\n      STATEFP == \"22\" ~ \"louisiana\",\n      STATEFP == \"23\" ~ \"maine\",\n      STATEFP == \"24\" ~ \"maryland\",\n      STATEFP == \"25\" ~ \"massachusetts\",\n      STATEFP == \"26\" ~ \"michigan\",\n      STATEFP == \"27\" ~ \"minnesota\",\n      STATEFP == \"28\" ~ \"mississippi\",\n      STATEFP == \"29\" ~ \"missouri\",\n      STATEFP == \"30\" ~ \"montana\",\n      STATEFP == \"31\" ~ \"nebraska\",\n      STATEFP == \"32\" ~ \"nevada\",\n      STATEFP == \"33\" ~ \"new hampshire\",\n      STATEFP == \"34\" ~ \"new jersey\",\n      STATEFP == \"35\" ~ \"new mexico\",\n      STATEFP == \"36\" ~ \"new york\",\n      STATEFP == \"37\" ~ \"north carolina\",\n      STATEFP == \"38\" ~ \"north dakota\",\n      STATEFP == \"39\" ~ \"ohio\",\n      STATEFP == \"40\" ~ \"oklahoma\",\n      STATEFP == \"41\" ~ \"oregon\",\n      STATEFP == \"42\" ~ \"pennsylvania\",\n      STATEFP == \"44\" ~ \"rhode island\",\n      STATEFP == \"45\" ~ \"south carolina\",\n      STATEFP == \"46\" ~ \"south dakota\",\n      STATEFP == \"47\" ~ \"tennessee\",\n      STATEFP == \"48\" ~ \"texas\",\n      STATEFP == \"49\" ~ \"utah\",\n      STATEFP == \"50\" ~ \"vermont\",\n      STATEFP == \"51\" ~ \"virginia\",\n      STATEFP == \"53\" ~ \"washington\",\n      STATEFP == \"54\" ~ \"west virginia\",\n      STATEFP == \"55\" ~ \"wisconsin\",\n      STATEFP == \"56\" ~ \"wyoming\",\n      TRUE ~ NA_character_\n    )\n  )\n\n\nReading layer `cb_2023_us_county_500k' from data source \n  `D:\\STA9750-2025-SPRING\\data\\mp04\\cb_2023_us_county_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3235 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n\n\nCode\nresults_2020 &lt;- bind_rows(results_2020)\n# Prepare 2020 and 2024 results with clean names\nresults_2020 &lt;- results_2020 |&gt;\n  mutate(\n    county_clean = tolower(str_replace_all(county, \"[^a-z]\", \"\")),\n    state_clean = tolower(state)\n  )\n\nresults_2024 &lt;- election_results_2024_final |&gt;\n  mutate(\n    county_clean = tolower(str_replace_all(county, \"[^a-z]\", \"\")),\n    state_clean = tolower(state)\n  )\n\n# Combine all datasets into one spatial object\nelections_combined &lt;- county_shapes |&gt;\n  left_join(results_2020, by = c(\"state_clean\", \"county_clean\"), suffix = c(\"\", \"_2020\")) |&gt;\n  left_join(results_2024, by = c(\"state_clean\", \"county_clean\"), suffix = c(\"_2020\", \"_2024\"))\n\n# Remove all-NA columns if needed\nelections_combined &lt;- elections_combined |&gt;\n  select(where(~ !all(is.na(.))))\n\nelections_combined &lt;- elections_combined |&gt;\n  mutate(donald_trump_republican_2024 = as.numeric(str_replace_all(donald_trump_republican_2024, \",\", \"\")))\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    joe_biden_democratic = as.numeric(str_replace_all(joe_biden_democratic, \",\", \"\")),\n    total_2020 = as.numeric(str_replace_all(total_2020, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    donald_trump_republican_2020 = as.numeric(str_replace_all(donald_trump_republican_2020, \",\", \"\")),\n    donald_trump_republican_2024 = as.numeric(str_replace_all(donald_trump_republican_2024, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    total_2020 = as.numeric(str_replace_all(total_2020, \",\", \"\")),\n    total_2024 = as.numeric(str_replace_all(total_2024, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    total_2020 = as.numeric(str_replace_all(as.character(total_2020), \",\", \"\")),\n    total_2024 = as.numeric(str_replace_all(as.character(total_2024), \",\", \"\"))\n  )\n\nView(elections_combined)\n\nelections_combined &lt;- read_csv(\"elections_combined.csv\")\nelections_combined &lt;- elections_combined |&gt;\n  left_join(\n    county_shapes |&gt; \n      st_drop_geometry() |&gt; \n      select(fips, ALAND, AWATER),\n    by = \"fips\"\n  )\nelections_combined &lt;- elections_combined |&gt;\n  janitor::clean_names() |&gt;\n  select(\n    county = name,            \n    statefp = statefp,       \n    state = state_clean,    \n    donald_trump_republican_2020,\n    joe_biden_democratic,\n    total_2020,\n    donald_trump_republican_2024,\n    kamala_harris_democratic,\n    total_2024,\n    geometry,\n    aland,                    \n    awater,\n    fips\n  )\nglimpse(elections_combined)\n\n\nRows: 3,267\nColumns: 13\n$ county                       &lt;chr&gt; \"Baldwin\", \"Houston\", \"Barbour\", \"Sumter\"‚Ä¶\n$ statefp                      &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"05\", \"05\", \"05\",‚Ä¶\n$ state                        &lt;chr&gt; \"alabama\", \"alabama\", \"alabama\", \"alabama‚Ä¶\n$ donald_trump_republican_2020 &lt;dbl&gt; 83544, 32618, 5622, 1598, 11920, 3884, 72‚Ä¶\n$ joe_biden_democratic         &lt;dbl&gt; 24578, 12917, 4816, 4648, 4245, 1116, 455‚Ä¶\n$ total_2020                   &lt;dbl&gt; 109679, 46173, 10518, 6291, 16529, 5202, ‚Ä¶\n$ donald_trump_republican_2024 &lt;dbl&gt; 95798, 32469, 5606, 1542, 11842, 3772, 69‚Ä¶\n$ kamala_harris_democratic     &lt;dbl&gt; 24934, 11352, 4158, 3725, 3769, 862, 3574‚Ä¶\n$ total_2024                   &lt;dbl&gt; 122249, 44349, 9855, 5307, 15803, 4714, 1‚Ä¶\n$ geometry                     &lt;chr&gt; \"list(list(c(-88.02858, -88.023991, -88.0‚Ä¶\n$ aland                        &lt;dbl&gt; 4117725048, 1501742235, 2292160151, 23408‚Ä¶\n$ awater                       &lt;dbl&gt; 1132887203, 4795415, 50523213, 24634880, ‚Ä¶\n$ fips                         &lt;chr&gt; \"01003\", \"01069\", \"01005\", \"01119\", \"0509‚Ä¶"
  },
  {
    "objectID": "mp04.html#initial-analysis",
    "href": "mp04.html#initial-analysis",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nNow, we will do some initial analysis of questions we wanted to answer from the data we gathered.\n1. Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2024)) |&gt;\n  slice_max(donald_trump_republican_2024, n = 5) |&gt;\n  select(state, county, donald_trump_republican_2024) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Trump Votes in 2024\")\n\n\n\nTop 5 Counties by Trump Votes in 2024\n\n\nstate\ncounty\ndonald_trump_republican_2024\n\n\n\n\ncalifornia\nLos Angeles\n1189862\n\n\ntexas\nHarris\n722695\n\n\ncalifornia\nOrange\n654815\n\n\ncalifornia\nSan Diego\n593270\n\n\ncalifornia\nRiverside\n463677\n\n\n\n\n\n2. Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(joe_biden_democratic), !is.na(total_2020)) |&gt;\n  mutate(biden_pct_2020 = joe_biden_democratic / total_2020) |&gt;\n  slice_max(biden_pct_2020, n = 5) |&gt;\n  select(state, county, biden_pct_2020) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Biden's 2020 Vote Share\")\n\n\n\nTop 5 Counties by Biden‚Äôs 2020 Vote Share\n\n\nstate\ncounty\nbiden_pct_2020\n\n\n\n\ncalifornia\nSan Francisco\n0.8525610\n\n\nmississippi\nJefferson\n0.8513306\n\n\nmississippi\nClaiborne\n0.8478310\n\n\ncalifornia\nMarin\n0.8232561\n\n\nalabama\nMacon\n0.8148573\n\n\n\n\n\n3.County with largest shift towards Trump (absolute increase in votes)\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2020), !is.na(donald_trump_republican_2024)) |&gt;\n  mutate(trump_vote_change = donald_trump_republican_2024 - donald_trump_republican_2020) |&gt;\n  slice_max(trump_vote_change, n = 5) |&gt;\n  select(state, county, trump_vote_change) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Increase in Trump Votes from 2020 to 2024\")\n\n\n\nTop 5 Counties by Increase in Trump Votes from 2020 to 2024\n\n\nstate\ncounty\ntrump_vote_change\n\n\n\n\ncalifornia\nLos Angeles\n44332\n\n\ntexas\nBaylor\n39704\n\n\ntexas\nTaylor\n39704\n\n\ntexas\nBexar\n28927\n\n\ntexas\nMontgomery\n28582\n\n\n\n\n\n4. Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024? (Note that the total votes for a state can be obtained by summing all counties in that state.)\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2020), !is.na(donald_trump_republican_2024)) |&gt;\n  group_by(state) |&gt;\n  summarise(\n    trump_2020 = sum(donald_trump_republican_2020, na.rm = TRUE),\n    trump_2024 = sum(donald_trump_republican_2024, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(trump_shift = trump_2024 - trump_2020) |&gt;\n  slice_min(trump_shift, n = 5) |&gt;\n  knitr::kable(caption = \"Top 5 States by Largest Decrease in Trump Votes from 2020 to 2024\")\n\n\n\nTop 5 States by Largest Decrease in Trump Votes from 2020 to 2024\n\n\nstate\ntrump_2020\ntrump_2024\ntrump_shift\n\n\n\n\noregon\n958448\n919480\n-38968\n\n\nindiana\n1729863\n1720347\n-9516\n\n\nmississippi\n756764\n747744\n-9020\n\n\nhawaii\n196865\n193664\n-3201\n\n\narkansas\n760647\n759241\n-1406\n\n\n\n\n\n5. What is the largest county, by area, in this data set?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(aland)) |&gt;\n  slice_max(aland, n = 5) |&gt;\n  select(county, state, aland) |&gt;\n  knitr::kable(caption = \"Top 5 Largest Counties by Area (ALAND)\")\n\n\n\nTop 5 Largest Counties by Area (ALAND)\n\n\ncounty\nstate\naland\n\n\n\n\nYukon-Koyukuk\nalaska\n377055293513\n\n\nNorth Slope\nalaska\n230052121425\n\n\nBethel\nalaska\n105253430580\n\n\nNorthwest Arctic\nalaska\n92367472818\n\n\nSoutheast Fairbanks\nalaska\n64315282486\n\n\n\n\n\n6. Which county has the highest voter density (voters per unit of area) in 2020?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(total_2020), !is.na(aland), aland &gt; 0) |&gt;\n  mutate(voter_density_2020 = total_2020 / aland) |&gt;\n  slice_max(voter_density_2020, n = 5) |&gt;\n  select(county, state, voter_density_2020) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Voter Density in 2020 (Votes per Sq. Meter)\")\n\n\n\nTop 5 Counties by Voter Density in 2020 (Votes per Sq. Meter)\n\n\ncounty\nstate\nvoter_density_2020\n\n\n\n\nSan Francisco\ncalifornia\n0.0036672\n\n\nSt.¬†Louis\nmissouri\n0.0033559\n\n\nOrange\ncalifornia\n0.0007412\n\n\nDallas\ntexas\n0.0004079\n\n\nSt.¬†Louis\nmissouri\n0.0004077\n\n\n\n\n\n7. Which county had the largest increase in voter turnout in 2024?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(total_2020), !is.na(total_2024)) |&gt;\n  mutate(turnout_increase = total_2024 - total_2020) |&gt;\n  arrange(desc(turnout_increase)) |&gt;\n  select(county, state, total_2020, total_2024, turnout_increase) |&gt;\n  slice_head(n = 5) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Increase in Voter Turnout from 2020 to 2024\")\n\n\n\nTop 5 Counties by Increase in Voter Turnout from 2020 to 2024\n\n\ncounty\nstate\ntotal_2020\ntotal_2024\nturnout_increase\n\n\n\n\nBaylor\ntexas\n1702\n55417\n53715\n\n\nTaylor\ntexas\n1702\n55417\n53715\n\n\nWarren\nkentucky\n20064\n56891\n36827\n\n\nBarren\nkentucky\n20064\n56891\n36827\n\n\nMontgomery\ntexas\n271543\n307258\n35715"
  },
  {
    "objectID": "mp04.html#electorial-shift-from-2020-to-2024",
    "href": "mp04.html#electorial-shift-from-2020-to-2024",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Electorial Shift from 2020 to 2024",
    "text": "Electorial Shift from 2020 to 2024\nWe created a graph of the United States that how to see the political shift from the 2020 election to the 2024 election.\n\n\nCode\nif (!exists(\"elections_combined\")) {\n  elections_combined &lt;- read_csv(\"data/mp04/elections_combined.csv\")\n}\n\nelection_shifts &lt;- elections_combined |&gt;\n  mutate(\n    trump_pct_2020 = donald_trump_republican_2020 / total_2020,\n    biden_pct_2020 = joe_biden_democratic / total_2020,\n    trump_pct_2024 = donald_trump_republican_2024 / total_2024,\n    harris_pct_2024 = kamala_harris_democratic / total_2024,\n    margin_2020 = trump_pct_2020 - biden_pct_2020,\n    margin_2024 = trump_pct_2024 - harris_pct_2024,\n    margin_shift = margin_2024 - margin_2020,\n    margin_shift_pct = margin_shift * 100\n  ) |&gt;\n  filter(!is.na(margin_shift))\n\n\nif (!exists(\"county_shapes\")) {\n  county_shapes &lt;- st_read(\"data/mp04/cb_2023_us_county_500k.shp\")\n}\n\ncounty_shapes &lt;- county_shapes |&gt;\n  mutate(fips = paste0(STATEFP, COUNTYFP))\n\n\ncounties_with_shifts &lt;- county_shapes |&gt;\n  left_join(election_shifts |&gt;\n              select(fips, margin_shift, margin_shift_pct),\n            by = \"fips\")\n\nstates &lt;- counties_with_shifts |&gt;\n  group_by(STATEFP) |&gt;\n  summarise(geometry = st_union(geometry), .groups = \"drop\") |&gt;\n  st_as_sf()\n\nmove_ak &lt;- function(geom) {\n  geom_scaled &lt;- (geom - st_centroid(geom)) * 0.4  # scale around centroid\n  geom_shifted &lt;- geom_scaled + c(-2500000, -1300000)\n  return(geom_shifted)\n}\n\nmove_hi &lt;- function(geom) {\n  geom_shifted &lt;- geom + c(-1000000, -1400000)\n  return(geom_shifted)\n}\nstates_transformed &lt;- states |&gt;\n  st_transform(5070)\n\nstates_transformed$geometry[states_transformed$STATEFP == \"02\"] &lt;- move_ak(states_transformed$geometry[states_transformed$STATEFP == \"02\"])\nstates_transformed$geometry[states_transformed$STATEFP == \"15\"] &lt;- move_hi(states_transformed$geometry[states_transformed$STATEFP == \"15\"])\n\ncounties_transformed &lt;- counties_with_shifts |&gt;\n  st_transform(5070)\n\ncounties_transformed$geometry[counties_transformed$STATEFP == \"02\"] &lt;- move_ak(counties_transformed$geometry[counties_transformed$STATEFP == \"02\"])\ncounties_transformed$geometry[counties_transformed$STATEFP == \"15\"] &lt;- move_hi(counties_transformed$geometry[counties_transformed$STATEFP == \"15\"])\narrow_scale &lt;- 20000\n\ncounties_arrows &lt;- counties_transformed |&gt;\n  filter(!is.na(margin_shift)) |&gt;\n  mutate(\n    centroid = st_centroid(geometry),\n    x = st_coordinates(centroid)[, 1],\n    y = st_coordinates(centroid)[, 2],\n    angle = if_else(margin_shift &gt; 0, 0, pi),\n    length = abs(margin_shift) * arrow_scale,\n    x_end = x + cos(angle) * length,\n    y_end = y + sin(angle) * length\n  )\nggplot() +\n  geom_sf(data = states_transformed, fill = \"white\", color = \"gray70\", size = 0.2) +\n  geom_segment(\n    data = counties_arrows,\n    aes(x = x, y = y, xend = x_end, yend = y_end, color = margin_shift &gt; 0),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3,\n    alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"blue\"),\n    labels = c(\"TRUE\" = \"More Rep.\", \"FALSE\" = \"More Dem.\"),\n    name = \"Shift from 2020 to 2024\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"County-Level Shift in Vote Margin: 2020 ‚Üí 2024\",\n    caption = \"Data: Wikipedia county election results\"\n  ) +\n  coord_sf(\n    crs = st_crs(counties_transformed),\n    xlim = c(-2500000, 2500000),  # tweak these numbers\n    ylim = c(-1000000, 3500000),  # tweak these numbers\n    expand = FALSE\n  )\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"nyt_county_shift_map_large.png\", width = 18, height = 11, dpi = 300)"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "",
    "text": "The Green Transit Alliance for Investigation of Variance (GTA IV) is proud to present this years Greener America Awards, where we will be awarding public transit systems all across America for their contribution to environmental conciousness.\n1"
  },
  {
    "objectID": "mp02.html#executive-summary",
    "href": "mp02.html#executive-summary",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe US Public Transit System has always been the backbone of transportation in America, serving millions of people everyday all while staying aware of environmental hazards surrounding the use of transportation such as carbon emissions or air pollution. Today, the Green Transit Alliance for Investigation of Variance will be giving awards to honor and congratulate many public transit systems for their commitment to sustainability. We hope that this will inspire more transit systems in America and all around the world to keep the environment sustainable and create awareness on the matter.\n\nThe smaller agencies out there will not go unnoticed as well, here in GTA IV we have calculated ways to award not only the largest and most used public transits, but also small and medium sized ones for their hardwork in preservation to be recognized.\n\nNow, drumroll please as we start the awards‚Ä¶‚Ä¶ü•Åü•Åü•Å"
  },
  {
    "objectID": "mp02.html#greenest-transit-agency-award",
    "href": "mp02.html#greenest-transit-agency-award",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Greenest Transit Agency Award",
    "text": "Greenest Transit Agency Award\n\nThe winners of the Greenest Transit Agency Award are‚Ä¶.\n\nSmall Agency: City Of Long Beach, with an emission of 0.0765 kilograms of C02 per UPT, with a median score of 2.77 kg of C02 per UPT for small agencies.2\nMedium Agency: Whatcom Transportation Authority, with an emission of 0.0242 kilograms of C02 per UPT, with a median of 1.33 kg of C02 per UPT for medium agencies.\nLarge Agency: Central Florida Regional Transportation Authority, with an emission of 0.000456 kilograms of C02 per UPT, with a median of 0.37 kg of C02 per UPT for large agencies.\n\nThe GTA IV calculated these statistics by computing the total kilograms of C02 emitted per gallon of different gases burned and then divided the total emissions by UPT to find which agencies emit the least amount of C02 by UPT."
  },
  {
    "objectID": "mp02.html#most-emissions-avoided-award",
    "href": "mp02.html#most-emissions-avoided-award",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Most Emissions Avoided Award",
    "text": "Most Emissions Avoided Award\n\nThe winners of the Most Emissions Avoided Award are‚Ä¶\nSmall Agency: Trans-Bridge Lines, Inc. with 2925138 kilograms of C02 emissions avoided compared to the average standard, with the median emissions avoided being -303903.6 kg of C02 emissions for small agencies.\nMedium Agency: Intercity Transit with 9092042 kilograms of C02 emission avoided compared to the average standard, with the median emissions avoided being -618120.8 kg of C02 emission for medium agencies.\nLarge Agency: MTA New York City Transit with 7299011485 kilograms of C02 emission avoided compared to the average standard, with the median emissions avoided being 7487465.4 kg of C02 emission for large agencies.\n\nThe GTA IV calculated these with the average miles per gallon being around 49 and the average amount of c02 being emitted per gallon being 8.887. Using these metrics, we calculated the hypothetical emissions and subtracted it from the total emissions to compare the total amount of emissions avoided."
  },
  {
    "objectID": "mp02.html#gas-guzzlers-award",
    "href": "mp02.html#gas-guzzlers-award",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Gas Guzzlers Award",
    "text": "Gas Guzzlers Award\n\nWhile we want to recognize the public transit systems that are keeping the environment sustainable, it is also important to look at the ones that have the most inefficient fuel use and encourage them to do better in the future! The Gas Guzzler award is just that, and hopefully these transit systems are able to improve moving forward. The award goes to‚Ä¶\n\nSmall Agency: Valley Regional Transit, with 4.59 kilograms of C02 emission per mile, with the median emissions avoided being -303903.6 kg of C02 emissions for small agencies.\nMedium Agency: Mid Mon Valley Transit Authority with 4.59 kilograms of C02 emission per mile, with the median emissions avoided being -618120.8 kg of C02 emission for medium agencies.\nLarge Agency: Central Ohio Transit Authroity with 2.61 kilograms of C02 emission per mile, with the median emissions avoided being 7487465.4 kg of C02 emission for large agencies.\n\nThe GTA IV calculated these by dividing the total emissions of all agencies by total amount of miles traveled by the agency. This allowed us to see which agencies in each size bracket emits the most per mile."
  },
  {
    "objectID": "mp02.html#thank-you",
    "href": "mp02.html#thank-you",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Thank You!",
    "text": "Thank You!\nThese are all the awards we have for this year, but don‚Äôt worry, there will be more next year! Hopefully these awards will motivate more public transit agencies to become environmentally conscious and help with sustainability for years to come."
  },
  {
    "objectID": "mp02.html#appendix",
    "href": "mp02.html#appendix",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Appendix",
    "text": "Appendix\n\nData Acquisition\nThe data sources used to obtain and examine the data are the following:\n1. The US Electricity Profile 2023 from the U.S. Energy Information Administration\n2. The 2023 Annual Database Energy Consumption from the Federal Transit Administration\n3.The 2023 Service by Agency Report from the U.S. Office of Personnel Management\n4.The Carbon Dioxide Emissions Coefficients from the U.S. Energy Information Administration\n\n\n\nInitial Analysis and Organization\n\nEIA State Electricity Profiles\n\n\nCode\nensure_package(dplyr)\nensure_package(stringr)\nensure_package(tidyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(readxl)\n\nget_eia_sep &lt;- function(state, abbr){\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  \n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  \n  dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n  \n  if(!file.exists(file_name)){\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; \n      req_url_path(\"electricity\", \"state\", state_formatted)\n    \n    RESPONSE &lt;- req_perform(REQUEST)\n    \n    resp_check_status(RESPONSE)\n    \n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if(\"U.S. rank\" %in% colnames(TABLE)){\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  CO2_MWh &lt;- TABLE |&gt; \n    filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n    pull(Value) |&gt; \n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  PRIMARY &lt;- TABLE |&gt; \n    filter(Item == \"primary energy source\") |&gt; \n    pull(Rank)\n  \n  RATE &lt;- TABLE |&gt;\n    filter(Item == \"average retail price (cents/kwh)\") |&gt;\n    pull(Value) |&gt;\n    as.numeric()\n  \n  GENERATION_MWh &lt;- TABLE |&gt;\n    filter(Item == \"net generation (megawatthours)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  data.frame(CO2_MWh               = CO2_MWh, \n             primary_source        = PRIMARY,\n             electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n             # * 1000 kWh to MWH \n             generation_MWh        = GENERATION_MWh, \n             state                 = state, \n             abbreviation          = abbr\n  )\n}\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\nensure_package(scales)\nensure_package(DT)\n\nEIA_SEP_REPORT |&gt; \n    select(-abbreviation) |&gt;\n    arrange(desc(CO2_MWh)) |&gt;\n    mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n           electricity_price_MWh = dollar(electricity_price_MWh), \n           generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n    rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n           `Primary Source of Electricity Generation`=primary_source, \n           `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n           `Total Generation Capacity (MWh)`= generation_MWh, \n           State=state) |&gt;\n    datatable()\n\n\n\n\n\n\nQuestion 1: Which state has the most expensive retail electricity?\nFrom the table, we can see Hawaii has the most expensive retail electricity costing an average of $386 per 1000 kWh.\n\nQuestion 2: Which state has the ‚Äòdirtiest‚Äô electricity mix?\nFrom the table, we can see the state with the dirtiest electricity mix is West Virginia, with 1925 pounds of CO2 emitted per MWh of electricity produced\n\nQuestion 3: On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\n\n\n\nCode\naverage_CO2_per_MWh &lt;- EIA_SEP_REPORT |&gt; \n  summarise(weighted_avg_CO2 = sum(CO2_MWh * generation_MWh) / sum(generation_MWh))\naverage_CO2_per_MWh |&gt; \n  kable(caption = \"Average CO2 Emissions per MWh\")\n\n\n\nAverage CO2 Emissions per MWh\n\n\nweighted_avg_CO2\n\n\n\n\n805.3703\n\n\n\n\n\nQuestion 4: What is the rarest primary energy source in the US? What is the associated cost of electricity and where is it used?\n\n\n\nCode\nrarest_energy_source &lt;- EIA_SEP_REPORT |&gt; \n  group_by(primary_source) |&gt; \n  summarise(total_generation = sum(generation_MWh, na.rm = TRUE)) |&gt; \n  filter(total_generation == min(total_generation, na.rm = TRUE))\nrarest_energy_source |&gt; \n  kable(caption = \"Rarest Energy Source by Total Generation\")\n\n\n\nRarest Energy Source by Total Generation\n\n\nprimary_source\ntotal_generation\n\n\n\n\nPetroleum\n9194164\n\n\n\n\n\nQuestion 5:Texas has a reputation as being the home of ‚Äúdirty fossil fuels‚Äù while NY has a reputation as a leader in clean energy. How many times cleaner is NY‚Äôs energy mix than that of Texas?\n\n\n\nCode\nny_cleanliness &lt;- EIA_SEP_REPORT |&gt; \n  filter(state == \"New York\") |&gt; \n  pull(CO2_MWh)\n\ntx_cleanliness &lt;- EIA_SEP_REPORT |&gt; \n  filter(state == \"Texas\") |&gt; \n  pull(CO2_MWh)\nclean_ratio &lt;- tx_cleanliness / ny_cleanliness\nclean_ratio_df &lt;- data.frame(State_Comparison = c(\"Texas vs New York\"), Clean_Ratio = clean_ratio)\nclean_ratio_df |&gt; \n  kable(caption = \"CO2 per MWh Ratio: Texas vs New York\")\n\n\n\nCO2 per MWh Ratio: Texas vs New York\n\n\nState_Comparison\nClean_Ratio\n\n\n\n\nTexas vs New York\n1.637931\n\n\n\n\n\n\n\n2023 Annual Database Energy Consumption\n\n\nCode\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\n\nWarning: Expecting numeric in R1197 / R1197C18: got '-'\n\n\nCleaning data and updating all the modes\n\n\nCode\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n    x &lt;- if_else(x == \"-\", NA, x)\n    replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\n\n2023 Annual Database service by Agency\nImporting and cleaning data\n\n\nCode\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                      destfile=NTD_SERVICE_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\n\nRows: 1000 Columns: 35\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (8): _5_digit_ntd_id, agency, max_reporter_type, max_organization_type,...\ndbl (27): report_year, max_agency_voms, max_primary_uza_area_sq_miles, max_p...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\n\nQuestion 1:Which transit service has the most UPT annually?\n\n\n\nCode\nmost_upt_service &lt;- NTD_SERVICE |&gt;\n  arrange(desc(UPT)) |&gt;\n  slice(1) |&gt;\n  select(Agency, City, State, UPT)\nmost_upt_service |&gt; \n  kable(caption = \"Agency with the Most UPT\")\n\n\n\nAgency with the Most UPT\n\n\nAgency\nCity\nState\nUPT\n\n\n\n\nMTA New York City Transit\nBrooklyn\nNY\n2632003044\n\n\n\n\n\nQuestion 2:What is the average trip length of a trip on MTA NYC?\n\n\nCode\nmta_nyc_avg_trip_length &lt;- NTD_SERVICE |&gt;\n  filter(grepl(\"MTA\", Agency)) |&gt;  \n  mutate(avg_trip_length = MILES / UPT) |&gt;\n  summarize(avg_trip_length = mean(avg_trip_length, na.rm = TRUE))\nmta_nyc_avg_trip_length |&gt; \n  kable(caption = \"Average Trip Length for MTA NYC\")\n\n\n\nAverage Trip Length for MTA NYC\n\n\navg_trip_length\n\n\n\n\n10.71179\n\n\n\n\n\nQuestion 3:Which transit service in NYC has the longest average trip length?\n\n\nCode\nnyc_longest_trip &lt;- NTD_SERVICE |&gt;\n  filter(City %in% c(\"New York City\", \"Brooklyn\")) |&gt;\n  mutate(avg_trip_length = MILES / UPT) |&gt;\n  arrange(desc(avg_trip_length)) |&gt;\n  slice(1) |&gt;\n  select(Agency, City, avg_trip_length)\nnyc_longest_trip |&gt; \n  kable(caption = \"City with the Longest Average Trip Length in NYC\")\n\n\n\nCity with the Longest Average Trip Length in NYC\n\n\nAgency\nCity\navg_trip_length\n\n\n\n\nPrivate Transportation Corporation\nBrooklyn\n5.233385\n\n\n\n\n\nQuestion 4:Which state has the fewest total miles travelled by public transit?\n\n\nCode\nstate_fewest_miles &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarize(total_miles = sum(MILES, na.rm = TRUE)) |&gt;\n  arrange(total_miles) |&gt;\n  slice(1)\nstate_fewest_miles |&gt;\n  kable(caption = \"State with the Fewest Total Miles\")\n\n\n\nState with the Fewest Total Miles\n\n\nState\ntotal_miles\n\n\n\n\nNH\n3749892\n\n\n\n\n\nQuestion 5:Are all states represented in this data?\n\n\nCode\nall_states &lt;- state.abb \nmissing_states &lt;- setdiff(all_states, unique(NTD_SERVICE$State))\n\nmissing_states |&gt;\n  kable(caption = \"States Missing from the NTD_Service Dataset\")\n\n\n\nStates Missing from the NTD_Service Dataset\n\n\nx\n\n\n\n\nAZ\n\n\nAR\n\n\nCA\n\n\nCO\n\n\nHI\n\n\nIA\n\n\nKS\n\n\nLA\n\n\nMO\n\n\nMT\n\n\nNE\n\n\nNV\n\n\nNM\n\n\nND\n\n\nOK\n\n\nSD\n\n\nTX\n\n\nUT\n\n\nWY\n\n\n\n\n\n\n\nJoining Datasets\n\n\nCode\ncolnames(NTD_SERVICE)[which(names(NTD_SERVICE) == \"Agency\")] &lt;- \"Agency Name\"\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT |&gt; select(-state)\nNTD_SERVICE &lt;- NTD_SERVICE |&gt; mutate(State = as.character(State))\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT |&gt; mutate(abbreviation = as.character(abbreviation))\ncombined_data &lt;- NTD_SERVICE |&gt; \n  inner_join(NTD_ENERGY, by = c(\"NTD ID\", \"Agency Name\"))\nfinal_data &lt;- combined_data |&gt; \n  left_join(EIA_SEP_REPORT, by = c(\"State\" = \"abbreviation\"))\nfinal_data |&gt; \n  head() |&gt;\n  kable(caption = \"Preview of Final Combined Data\")\n\n\n\nPreview of Final Combined Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgency Name\nCity\nState\nUPT\nMILES\nNTD ID\nMode\nBio-Diesel\nBunker Fuel\nC Natural Gas\nDiesel Fuel\nElectric Battery\nElectric Propulsion\nEthanol\nMethonal\nGasoline\nHydrogen\nKerosene\nLiquified Nat Gas\nLiquified Petroleum Gas\nCO2_MWh\nprimary_source\nelectricity_price_MWh\ngeneration_MWh\n\n\n\n\nSpokane Transit Authority\nSpokane\nWA\n9403739\n46318134\n2\nDemand Response\n0\n0\n0\n131642\n0\n0\n0\n0\n152360\n0\n0\n0\n0\n292\nHydroelectric\n95.8\n102960605\n\n\nSpokane Transit Authority\nSpokane\nWA\n9403739\n46318134\n2\nBus\n0\n0\n0\n1335531\n1202138\n0\n0\n0\n0\n0\n0\n0\n0\n292\nHydroelectric\n95.8\n102960605\n\n\nSpokane Transit Authority\nSpokane\nWA\n9403739\n46318134\n2\nVanpool\n0\n0\n0\n0\n0\n0\n0\n0\n56463\n0\n0\n0\n0\n292\nHydroelectric\n95.8\n102960605\n\n\nLane Transit District\nEugene\nOR\n6311613\n22779952\n7\nDemand Response\n0\n0\n0\n0\n0\n0\n0\n0\n94756\n0\n0\n0\n0\n344\nHydroelectric\n103.2\n61691869\n\n\nLane Transit District\nEugene\nOR\n6311613\n22779952\n7\nBus\n0\n0\n0\n450303\n920696\n0\n0\n0\n5806\n0\n0\n0\n0\n344\nHydroelectric\n103.2\n61691869\n\n\nLane Transit District\nEugene\nOR\n6311613\n22779952\n7\nBus Rapid Transit\n212371\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n344\nHydroelectric\n103.2\n61691869\n\n\n\n\n\nAfter joining the three tables, we used mutute to compute the total emissions for the row using data from the EIA website.\n\n\nCode\nfinal_data &lt;- final_data |&gt; \n  mutate(\n    total_emissions = (`Diesel Fuel` * 10.21) +\n      (`Gasoline` * 8.89) +\n      (`C Natural Gas` * 53.06) +\n      (`Liquified Petroleum Gas` * 5.79) +\n      (`Bio-Diesel` * 9.45) +\n      (`Ethanol` * 1.94) +\n      (`Hydrogen` * 0) +\n      (`Kerosene` * 9.75) +\n      (`Liquified Nat Gas` * 62.44) +\n      (`Bunker Fuel` * 10.96) +\n      (`Electric Battery` * CO2_MWh / 1000) +  # Convert to metric tons\n      (`Electric Propulsion` * CO2_MWh / 1000)\n  )\n\n\n\n\nNormalizing Emissions to Transit Usage\nWe categorized the agencies by size by the UPT of each transit system, with systems that have over 5000000 considered large, 500000 to 5000000 considered medium, and anything less is a small agency. We also calculated the emissions per UPT and emissions per mile by dividing total emissions by UPT and dividing total_emissions by total miles.\n\n\nCode\nfinal_data &lt;- final_data |&gt; \n  group_by(`Agency Name`) |&gt; \n  mutate(\n    emissions_per_UPT = total_emissions / UPT,\n    emissions_per_mile = total_emissions / MILES\n  ) |&gt; \n  ungroup()\nfinal_data &lt;- final_data |&gt; \n  group_by(`Agency Name`) |&gt; \n  mutate(\n    agency_size = case_when(\n      UPT &gt;= 5000000 ~ \"Large\",\n      UPT &gt;= 500000 ~ \"Medium\",\n      TRUE ~ \"Small\"\n    )\n  ) |&gt; \n  ungroup()\n\n\nCode for Emissions per UPT:\n\n\nCode\nmost_efficient &lt;- final_data |&gt; \n  group_by(agency_size, `Agency Name`) |&gt; \n  summarize(\n    avg_emissions_per_UPT = mean(emissions_per_UPT, na.rm = TRUE),\n    avg_emissions_per_mile = mean(emissions_per_mile, na.rm = TRUE)\n  ) |&gt; \n  arrange(agency_size, avg_emissions_per_UPT, avg_emissions_per_mile)\n\n\n`summarise()` has grouped output by 'agency_size'. You can override using the\n`.groups` argument.\n\n\nCode\nbest_efficiency &lt;- most_efficient |&gt; \n  group_by(agency_size) |&gt; \n  slice_min(order_by = avg_emissions_per_UPT, n = 1)\n\n\nGreenest Agencies Calculation Code\nWe calculated this by filtering by emissions_per_UPT to see the lowest for every agency size to get the 3 greenest agencies.\n\n\nCode\nmedian_emissions &lt;- final_data |&gt;\n  group_by(agency_size) |&gt;\n  summarise(median_emissions_per_UPT = median(emissions_per_UPT, na.rm = TRUE))\ngreenest_agencies &lt;- final_data |&gt;\n  group_by(agency_size) |&gt;\n  filter(emissions_per_UPT == min(emissions_per_UPT, na.rm = TRUE)) |&gt;\n  ungroup()\ngreenest_agencies |&gt;\n  select(`Agency Name`, agency_size, emissions_per_UPT) |&gt;\n  left_join(median_emissions, by = \"agency_size\") |&gt;\n  kable(caption = \"Greenest Agencies and Median Emissions per UPT by Agency Size\")\n\n\n\nGreenest Agencies and Median Emissions per UPT by Agency Size\n\n\n\n\n\n\n\n\nAgency Name\nagency_size\nemissions_per_UPT\nmedian_emissions_per_UPT\n\n\n\n\nWhatcom Transportation Authority\nMedium\n0.0241804\n1.3337899\n\n\nCity of Long Beach\nSmall\n0.0764815\n2.7111502\n\n\nCentral Florida Regional Transportation Authority\nLarge\n0.0004559\n0.3708156\n\n\n\n\n\nCode\nmedian_emissions_fixed &lt;- median_emissions |&gt; distinct()\n\nggplot(greenest_agencies |&gt; left_join(median_emissions, by = \"agency_size\"), \n       aes(x = agency_size, y = emissions_per_UPT, fill = `Agency Name`)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_hline(data = median_emissions_fixed, aes(yintercept = median_emissions_per_UPT, color = agency_size), \n             linetype = \"dashed\", size = 1.2) +\n  scale_color_manual(values = c(\"Large\" = \"blue\", \"Medium\" = \"green\", \"Small\" = \"red\")) + \n  labs(title = \"Greenest Agencies vs. Median Emissions per UPT\",\n       x = \"Agency Size\", \n       y = \"Emissions per UPT\",\n       fill = \"Best Agency\",\n       color = \"Median Line\") + \n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nFrom the plot, we are able to see the median emission per UPT from the line, while the bars represent the agencies with the lowest emission, showing how big of an impact they are making with lower emissions. Most Emissions Avoided Award Calculation Code\nWe calculated this by finding the average mile per gallon of a vehicle and the C02 emission per gallon and found the hypothetical emission by diving miles by average mile per gallon and then multiplying it by galloon. We then subtracted the hypothetical emissions by total emissions to see which agencies saved the most.\n\n\nCode\nfuel_economy_mpg &lt;- 49\nco2_per_gallon &lt;- 8.887\nfinal_data &lt;- final_data |&gt;\n  mutate(hypothetical_emissions = (MILES / fuel_economy_mpg) * co2_per_gallon,\n         emissions_avoided = hypothetical_emissions - total_emissions)\nmost_emissions_avoided &lt;- final_data |&gt;\n  group_by(agency_size, `Agency Name`) |&gt;\n  summarise(total_emissions_avoided = sum(emissions_avoided, na.rm = TRUE)) |&gt;\n  arrange(desc(total_emissions_avoided)) |&gt;\n  slice(1)\n\n\n`summarise()` has grouped output by 'agency_size'. You can override using the\n`.groups` argument.\n\n\nCode\nmedian_emissions_avoided &lt;- final_data |&gt;\n  group_by(agency_size) |&gt;\n  summarise(median_emissions_avoided = median(emissions_avoided, na.rm = TRUE))\nfinal_results &lt;- most_emissions_avoided |&gt;\n  left_join(median_emissions_avoided, by = \"agency_size\")\nkable(final_results, caption = \"Agency with the Most Emissions Avoided and Median Emissions Avoided by Agency Size\")\n\n\n\nAgency with the Most Emissions Avoided and Median Emissions Avoided by Agency Size\n\n\n\n\n\n\n\n\nagency_size\nAgency Name\ntotal_emissions_avoided\nmedian_emissions_avoided\n\n\n\n\nLarge\nMTA New York City Transit\n7299011485\n7487465.4\n\n\nMedium\nIntercity Transit\n9092042\n-618120.8\n\n\nSmall\nTrans-Bridge Lines, Inc.\n2925138\n-303803.6\n\n\n\n\n\nGas Guzzlers Award Calculation Code\nWe calculated Gas Guzzlers by filtering to see which agencies emit the most C02 by mile.\n\n\nCode\ngas_guzzler_award &lt;- final_data |&gt;\n  group_by(agency_size) |&gt;\n  filter(emissions_per_mile == max(emissions_per_mile, na.rm = TRUE)) |&gt;\n  select(`Agency Name`, City, State, agency_size, emissions_per_mile, total_emissions)\nkable(gas_guzzler_award, caption = \"Gas Guzzler Award - Agency with Highest Emissions Per Mile\")\n\n\n\nGas Guzzler Award - Agency with Highest Emissions Per Mile\n\n\n\n\n\n\n\n\n\n\nAgency Name\nCity\nState\nagency_size\nemissions_per_mile\ntotal_emissions\n\n\n\n\nValley Regional Transit\nMeridian\nID\nMedium\n4.591114\n20562056\n\n\nMid Mon Valley Transit Authority\nCharleroi\nPA\nSmall\n4.586857\n10641127\n\n\nCentral Ohio Transit Authority\nColumbus\nOH\nLarge\n2.606710\n133676175\n\n\n\n\n\nCode\nggplot(final_data, aes(x = agency_size, y = emissions_per_UPT, fill = agency_size)) + \n  geom_boxplot() + \n  scale_y_log10() + \n  labs(title = \"Distribution of Emissions per UPT by Agency Size\", \n       x = \"Agency Size\", \n       y = \"Emissions per UPT (log scale)\") + \n  theme_minimal()\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWe had to scale the plot using log because the values for large agencies were too big, then we are able to visualize the distributions per UPT by agency size and the dots represent outliers in those plots. Based off the graph, we can see the highest points, lowest points, and the median."
  },
  {
    "objectID": "mp02.html#footnotes",
    "href": "mp02.html#footnotes",
    "title": "GTA IV - Greener America Awards 2025",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll images were created using Canva AI Image Generator‚Ü©Ô∏é\nUnlinked Passenger Trips, the distinct number of trips taken on public transit.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I‚Äôm Jackey",
    "section": "",
    "text": "Nice to meet you! My name is Jackey and I am currently a student at Baruch pursing a masters in Business Analytics as of Spring 2025 where I am developing skills in statistical analysis, data-driven decision-making, and business intellegence. I am passionate about applying analytical tools to optimize business performance and drive meaningful insights.\nMy hobbies include playing video games, rock climbing, reading books, and going to the gym.\nBelow I have attached my resume\nHere\n\nLast Updated: Wednesday 05 07, 2025 at 14:37PM"
  },
  {
    "objectID": "data/mp04/mp04.html",
    "href": "data/mp04/mp04.html",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "",
    "text": "With the recent presidential election that brought Trump back into office, we wanted to explore the data from the 2020 and 2024 elections and see the politial shift from 2020 to 2024."
  },
  {
    "objectID": "data/mp04/mp04.html#americas-political-shift",
    "href": "data/mp04/mp04.html#americas-political-shift",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "",
    "text": "With the recent presidential election that brought Trump back into office, we wanted to explore the data from the 2020 and 2024 elections and see the politial shift from 2020 to 2024."
  },
  {
    "objectID": "data/mp04/mp04.html#acquiring-data",
    "href": "data/mp04/mp04.html#acquiring-data",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Acquiring Data",
    "text": "Acquiring Data\nFor the analysis, we are focusing on the states and their counties, so we will be downloading and scraping data from the Census Bureau and Wikipedia.\n\nUS County Shapefile\nFirst, we wanted to download the US County Shapefile data from the Census Bureau website.\n\n\nCode\ndir_path &lt;- \"data/mp04\"\nzip_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.zip\")\nshapefile_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\"\nif (!dir_exists(dir_path)) {\n  dir_create(dir_path)\n}\nif (!file_exists(zip_file)) {\n  download.file(shapefile_url, destfile = zip_file, mode = \"wb\")\n}\nshp_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.shp\")\nif (!file_exists(shp_file)) {\n  unzip(zip_file, exdir = dir_path)\n}\ncounty_shapes &lt;- st_read(shp_file)\n\n\nReading layer `cb_2023_us_county_500k' from data source \n  `D:\\STA9750-2025-SPRING\\data\\mp04\\data\\mp04\\cb_2023_us_county_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3235 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n\n\nCode\ncounty_shapes |&gt;\n  slice_head(n = 10) |&gt; \n  st_drop_geometry() |&gt; \n  kable(caption = \"Preview of County Shapes Dataset\")\n\n\n\nPreview of County Shapes Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUNTYNS\nGEOIDFQ\nGEOID\nNAME\nNAMELSAD\nSTUSPS\nSTATE_NAME\nLSAD\nALAND\nAWATER\n\n\n\n\n01\n003\n00161527\n0500000US01003\n01003\nBaldwin\nBaldwin County\nAL\nAlabama\n06\n4117725048\n1132887203\n\n\n01\n069\n00161560\n0500000US01069\n01069\nHouston\nHouston County\nAL\nAlabama\n06\n1501742235\n4795415\n\n\n01\n005\n00161528\n0500000US01005\n01005\nBarbour\nBarbour County\nAL\nAlabama\n06\n2292160151\n50523213\n\n\n01\n119\n00161585\n0500000US01119\n01119\nSumter\nSumter County\nAL\nAlabama\n06\n2340898915\n24634880\n\n\n05\n091\n00069166\n0500000US05091\n05091\nMiller\nMiller County\nAR\nArkansas\n06\n1616257232\n36848741\n\n\n05\n133\n00069182\n0500000US05133\n05133\nSevier\nSevier County\nAR\nArkansas\n06\n1459636819\n45919661\n\n\n05\n093\n00069899\n0500000US05093\n05093\nMississippi\nMississippi County\nAR\nArkansas\n06\n2336409866\n72224609\n\n\n06\n037\n00277283\n0500000US06037\n06037\nLos Angeles\nLos Angeles County\nCA\nCalifornia\n06\n10515988121\n1785003256\n\n\n06\n087\n00277308\n0500000US06087\n06087\nSanta Cruz\nSanta Cruz County\nCA\nCalifornia\n06\n1152818089\n419720203\n\n\n06\n097\n01657246\n0500000US06097\n06097\nSonoma\nSonoma County\nCA\nCalifornia\n06\n4080103614\n497291856\n\n\n\n\n\n\n\n2024 County-Level Election Data\nFor this data, we will be web-scraping the 2024 presidential election results on a county level from Wikipedia. There are irregularities in some states which made it difficult to scrape the data, while we were able to write a function that automatically scraped a majority of the state‚Äôs data, we were missing Alaska because they do not have counties. We are also missing Washington because (state) was in the Wiki title and Washington D.C. is also missing because it is offically called ‚ÄúDistrict of Columbia‚Äù and they have ‚Äúwards‚Äù instead of counties.\n\n\nCode\nstate_names &lt;- c(state.name, \"District of Columbia\")\nwiki_titles &lt;- setNames(\n  paste0(\"2024_United_States_presidential_election_in_\", gsub(\" \", \"_\", state_names)),\n  state_names\n)\nwiki_titles[\"New York\"] &lt;- \"2024_United_States_presidential_election_in_New_York\"\nwiki_titles[\"Washington\"] &lt;- \"2024_United_States_presidential_election_in_Washington_(state)\"\nwiki_titles[\"District of Columbia\"] &lt;- \"2024_United_States_presidential_election_in_the_District_of_Columbia\"\n#function to download and clean data from wiki\ndir.create(\"data/mp04/state_pages\", showWarnings = FALSE, recursive = TRUE)\nget_state_results &lt;- function(state_name) {\n  state_url &lt;- gsub(\" \", \"_\", state_name)\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_url)\n  local_file &lt;- file.path(\"data/mp04/state_pages\", paste0(state_url, \".html\"))\n  if (!file.exists(local_file)) {\n    tryCatch({\n      resp &lt;- request(url) |&gt; req_perform()\n      writeLines(resp_body_string(resp), local_file)\n    }, error = function(e) {\n      message(\"Failed to download: \", state_name)\n      return(NULL)\n    })\n  }\n  page &lt;- tryCatch(read_html(local_file), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"‚ö†Ô∏è No county-level table for \", state_name)\n    return(NULL)\n  }\n  \n#Clean and standardize column names\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    mutate(state = state_name)\n  county_col &lt;- names(cleaned)[str_detect(names(cleaned), \"county|parish|borough\")][1]\n  if (!\"county\" %in% names(cleaned)) {\n    cleaned &lt;- cleaned |&gt; rename(county = all_of(county_col))\n  }\n  \n  return(cleaned)\n}\n#go through each state\nstate_names &lt;- state.name\nsafe_results &lt;- map(state_names, safely(get_state_results))\nresults &lt;- list()\nfailures &lt;- c()\n\nfor (i in seq_along(state_names)) {\n  result &lt;- safe_results[[i]]\n  if (is.null(result$result)) {\n    failures &lt;- c(failures, state_names[i])\n  } else {\n    results[[state_names[i]]] &lt;- result$result\n  }\n}\n\nelection_results_clean &lt;- bind_rows(results)\nelection_results_clean &lt;- election_results_clean |&gt;\n  select(where(~ !all(is.na(.)))) |&gt;  # drop all-NA columns\n  filter(!str_detect(county, regex(\"Total|Statewide|City|#\", ignore_case = TRUE))) |&gt;\n  mutate(across(where(is.character), ~ str_replace_all(.x, \"[,%]\", \"\")))  # remove commas/%\nwrite_csv(election_results_clean, \"data/mp04/combined_2024_county_results.csv\")\ncat(\"Failed States:\\n\")\n\n\nFailed States:\n\n\nCode\nprint(failures)\n\n\n[1] \"Alaska\"     \"Washington\"\n\n\nCode\nelection_results_clean &lt;- readr::read_csv(\"data/mp04/combined_2024_county_results.csv\")\nall_states &lt;- c(state.name, \"District of Columbia\")\nmissing_states &lt;- setdiff(all_states, unique(election_results_clean$state))\ncat(\"Missing States:\\n\")\n\n\nMissing States:\n\n\nCode\nprint(missing_states)\n\n\n[1] \"Alaska\"               \"Washington\"           \"District of Columbia\"\n\n\nTo find the data for all the missing states, we manually went into the page to see what was missing searched to see what tables we needed to add and hard coded the information.\n\n\nCode\n#ohio was missing so added code to add ohio\nget_ohio_results &lt;- function() {\n  state_name &lt;- \"Ohio\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Ohio\"\n  file_path &lt;- \"data/mp04/state_pages/Ohio.html\"\n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  # Read and parse\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  # Table 39 contains the data we want\n  county_table &lt;- tables[[39]]\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt;\n    mutate(state = state_name)\n  return(cleaned)\n}\nohio_results &lt;- get_ohio_results()\nohio_results &lt;- ohio_results |&gt;\n  mutate(\n    donald_trump_republican_2 = as.numeric(str_replace_all(donald_trump_republican_2, \"%\", \"\")),\n    kamala_harris_democratic_2 = as.numeric(str_replace_all(kamala_harris_democratic_2, \"%\", \"\")),\n    various_candidates_other_parties_2 = as.numeric(str_replace_all(various_candidates_other_parties_2, \"%\", \"\")),\n    margin_2 = as.numeric(str_replace_all(margin_2, \"%\", \"\"))\n  )\nelection_results_2024_updated &lt;- bind_rows(election_results_clean, ohio_results)\nwrite_csv(election_results_2024_updated, \"data/mp04/combined_2024_county_results.csv\")\n#adding washington\nget_washington_2024_results &lt;- function() {\n  state_name &lt;- \"Washington\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Washington_(state)\"\n  file_path &lt;- \"data/mp04/state_pages/Washington_2024.html\"\n  \n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  \n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  cleaned &lt;- county_table |&gt; \n    clean_names() |&gt; \n    mutate(state = state_name)\n  \n  return(cleaned)\n}\n\nwa_results &lt;- get_washington_2024_results()\n#adding dc\ndc_row &lt;- tibble(\n  county = \"District of Columbia\",\n  donald_trump_republican = 18000,  # replace with real estimate\n  donald_trump_republican_2 = 4.5,\n  kamala_harris_democratic = 378000,\n  kamala_harris_democratic_2 = 94.5,\n  various_candidates_other_parties = 4000,\n  various_candidates_other_parties_2 = 1.0,\n  margin = 360000,\n  margin_2 = 90,\n  total = 400000,\n  state = \"District of Columbia\"\n)\n#adding alaska\nak_row &lt;- tibble(\n  county = \"Statewide\",\n  donald_trump_republican = 184458,\n  donald_trump_republican_2 = 54.54,\n  kamala_harris_democratic = 140026,\n  kamala_harris_democratic_2 = 41.41,\n  various_candidates_other_parties = 14371,\n  various_candidates_other_parties_2 = 4.05,\n  margin = 44332,\n  margin_2 = 13.13,\n  total = 338855,\n  state = \"Alaska\"\n)\n#adding washington\nwa_results &lt;- wa_results |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\nak_row &lt;- ak_row |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\n\ndc_row &lt;- dc_row |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\n#combining all the data\nelection_results_2024_updated &lt;- election_results_2024_updated |&gt;\n  mutate(across(\n    c(\n      donald_trump_republican,\n      donald_trump_republican_2,\n      kamala_harris_democratic,\n      kamala_harris_democratic_2,\n      various_candidates_other_parties,\n      various_candidates_other_parties_2,\n      margin,\n      margin_2,\n      total\n    ),\n    ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))\n  ))\nelection_results_2024_final &lt;- bind_rows(\n  election_results_2024_updated,\n  dc_row,\n  ak_row,\n  wa_results\n)\nelection_results_2024_final &lt;- bind_rows(\n  election_results_2024_updated,\n  dc_row,\n  ak_row,\n  wa_results\n)\nelection_results_2024_final &lt;- election_results_2024_final |&gt;\n  filter(!str_detect(county, regex(\"^county(\\\\[\\\\d+\\\\])?$\", ignore_case = TRUE)))\nelection_results_2024_final |&gt;\n  head(10) |&gt;\n  kable(caption = \"Preview of 2024 Election Results\")\n\n\n\nPreview of 2024 Election Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\ndonald_trump_republican\ndonald_trump_republican_2\nkamala_harris_democratic\nkamala_harris_democratic_2\nvarious_candidates_other_parties\nvarious_candidates_other_parties_2\nmargin\nmargin_2\ntotal\nstate\ntotal_votes_cast\nkamala_harris_dfl\nkamala_harris_dfl_2\nkamala_harris_democratic_npl\nkamala_harris_democratic_npl_2\n\n\n\n\nAutauga\n20484\n72.43\n7439\n26.30\n358\n1.27\n13045\n46.13\n28281\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBaldwin\n95798\n78.36\n24934\n20.40\n1517\n1.24\n70864\n57.96\n122249\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBarbour\n5606\n56.88\n4158\n42.19\n91\n0.93\n1448\n14.69\n9855\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBibb\n7572\n81.80\n1619\n17.49\n66\n0.71\n5953\n64.31\n9257\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBlount\n25354\n90.03\n2576\n9.15\n233\n0.82\n22778\n80.88\n28163\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nBullock\n1101\n26.78\n2983\n72.56\n27\n0.66\n-1882\n-45.78\n4111\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nButler\n5172\n60.99\n3251\n38.34\n57\n0.67\n1921\n22.65\n8480\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nCalhoun\n34912\n71.76\n13194\n27.12\n547\n1.12\n21718\n44.64\n48653\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nChambers\n8711\n61.15\n5405\n37.94\n129\n0.91\n3306\n23.21\n14245\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\nCherokee\n11358\n87.33\n1553\n11.94\n95\n0.73\n9805\n75.39\n13006\nAlabama\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n2020 County-Level Election Data\nFor this data we will be doing the same downloading and scraping steps we did for the 2024 dataset.\n\n\nCode\ndir.create(\"data/mp04/state_pages_2020\", showWarnings = FALSE, recursive = TRUE)\ndir.create(\"data/mp04/state_pages_2020\", showWarnings = FALSE, recursive = TRUE)\nget_state_results_2020 &lt;- function(state_name) {\n  state_url &lt;- gsub(\" \", \"_\", state_name)\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_url)\n  local_file &lt;- file.path(\"data/mp04/state_pages_2020\", paste0(state_url, \".html\"))\n  if (!file.exists(local_file)) {\n    tryCatch({\n      resp &lt;- request(url) |&gt; req_perform()\n      writeLines(resp_body_string(resp), local_file)\n    }, error = function(e) {\n      message(\"Failed to download: \", state_name)\n      return(NULL)\n    })\n  }\n  page &lt;- tryCatch(read_html(local_file), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  county_table &lt;- tables |&gt;\n    keep(~ any(str_detect(tolower(names(.x)), \"county|parish|borough\"))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"‚ö†Ô∏è No county-level table for \", state_name)\n    return(NULL)\n  }\n  \n#Clean and standardize column names\n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    mutate(state = state_name)\n  county_col &lt;- names(cleaned)[str_detect(names(cleaned), \"county|parish|borough\")][1]\n  if (!\"county\" %in% names(cleaned)) {\n    cleaned &lt;- cleaned |&gt; rename(county = all_of(county_col))\n  }\n  \n  return(cleaned)\n}\n\nstate_names &lt;- state.name\nsafe_results_2020 &lt;- map(state_names, safely(get_state_results_2020))\nresults_2020 &lt;- list()\nfailures_2020 &lt;- c()\n\nfor (i in seq_along(state_names)) {\n  result &lt;- safe_results_2020[[i]]\n  if (is.null(result$result)) {\n    failures_2020 &lt;- c(failures_2020, state_names[i])\n  } else {\n    results_2020[[state_names[i]]] &lt;- result$result\n  }\n}\n\nelection_results_2020_clean &lt;- bind_rows(results_2020)\nelection_results_2020_clean &lt;- election_results_2020_clean |&gt;\n  select(where(~ !all(is.na(.)))) |&gt;  # drop all-NA columns\n  filter(!str_detect(county, regex(\"Total|Statewide|City|#\", ignore_case = TRUE))) |&gt;\n  mutate(across(where(is.character), ~ str_replace_all(.x, \"[,%]\", \"\")))  # remove commas/%\n\nwrite_csv(election_results_2020_clean, \"data/mp04/combined_2020_county_results.csv\")\n\n# Check which states are missing\nall_states &lt;- c(state.name, \"District of Columbia\")\nelection_results_2020_clean &lt;- readr::read_csv(\"data/mp04/combined_2020_county_results.csv\")\nmissing_states_2020 &lt;- setdiff(all_states, unique(election_results_2020_clean$state))\n\ncat(\"Missing 2020 States:\\n\")\n\n\nMissing 2020 States:\n\n\nCode\nprint(missing_states_2020)\n\n\n[1] \"Alaska\"               \"Washington\"           \"District of Columbia\"\n\n\nCode\nget_ohio_2020_results &lt;- function() {\n  state_name &lt;- \"Ohio\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Ohio\"\n  file_path &lt;- \"data/mp04/state_pages/Ohio_2020.html\"\n\n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  if (length(tables) &lt; 38) {\n    message(\"Table 398not found.\")\n    return(NULL)\n  }\n  \n  county_table &lt;- tables[[38]]\n  \n  cleaned &lt;- county_table |&gt;\n    clean_names() |&gt;\n    rename(county = 1) |&gt;  \n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt;\n    mutate(state = state_name)\n  \n  return(cleaned)\n}\nohio_2020_results &lt;- get_ohio_2020_results()\n#alaska 2020 info\nak_2020_row &lt;- tibble(\n  county = \"Statewide\",\n  donald_trump_republican = 189951,\n  joe_biden_democratic = 153778,\n  various_candidates_other_parties = 14917,\n  margin = 36173,\n  total = 358646,\n  state = \"Alaska\"\n)\n#washington 2020 info\nget_washington_2020_results &lt;- function() {\n  state_name &lt;- \"Washington\"\n  url &lt;- \"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_Washington_(state)\"\n  file_path &lt;- \"data/mp04/state_pages/Washington_2020.html\"\n  \n  if (!file.exists(file_path)) {\n    page &lt;- request(url) |&gt; req_perform()\n    writeLines(resp_body_string(page), file_path)\n  }\n  \n  page &lt;- read_html(file_path)\n  tables &lt;- page |&gt; html_elements(\"table\") |&gt; html_table()\n  \n  county_table &lt;- tables |&gt; \n    keep(~ any(str_detect(names(.x), regex(\"county\", ignore_case = TRUE)))) |&gt; \n    pluck(1, .default = NULL)\n  \n  if (is.null(county_table)) {\n    message(\"No Washington county-level table found.\")\n    return(NULL)\n  }\n  \n  cleaned &lt;- county_table |&gt; \n    clean_names() |&gt; \n    rename(county = 1) |&gt; \n    filter(!str_detect(county, regex(\"total|summary\", ignore_case = TRUE))) |&gt; \n    mutate(state = state_name)\n  \n  return(cleaned)\n}\nwa_2020_results &lt;- get_washington_2020_results()\n\n#dc 2020 info\ndc_2020_row &lt;- tibble(\n  county = \"District of Columbia\",\n  donald_trump_republican = 18000,\n  donald_trump_republican_2 = 5.4,\n  joe_biden_democratic = 317000,\n  joe_biden_democratic_2 = 92.1,\n  various_candidates_other_parties = 4000,\n  various_candidates_other_parties_2 = 2.5,\n  margin = 299000,\n  margin_2 = 86.7,\n  total = 344000,\n  state = \"District of Columbia\"\n)\ncols_to_fix &lt;- c(\n  \"donald_trump_republican\", \"donald_trump_republican_2\",\n  \"joe_biden_democratic\", \"joe_biden_democratic_2\",\n  \"various_candidates_other_parties\", \"various_candidates_other_parties_2\",\n  \"margin\", \"margin_2\", \"total\",\n  \"howie_hawkins_green\" \n)\n\nfix_numeric &lt;- function(df) {\n  df |&gt;\n    mutate(across(any_of(cols_to_fix), ~ as.numeric(str_replace_all(.x, \"[%,]\", \"\"))))\n}\n#combining all the data\nelection_results_2020 &lt;- fix_numeric(election_results_2020_clean)\nohio_2020_results &lt;- fix_numeric(ohio_2020_results)\nak_2020_row &lt;- fix_numeric(ak_2020_row)\nwa_2020_results &lt;- fix_numeric(wa_2020_results)\ndc_2020_row &lt;- fix_numeric(dc_2020_row)\nelection_results_2020_final &lt;- bind_rows(\n  election_results_2020,\n  ohio_2020_results,\n  ak_2020_row,\n  wa_2020_results,\n  dc_2020_row\n)\nelection_results_2020_final &lt;- election_results_2020_final |&gt;\n  filter(!str_detect(county, regex(\"^county(\\\\[\\\\d+\\\\])?$\", ignore_case = TRUE)))\nelection_results_2020_final &lt;- election_results_2020_final |&gt;\n  select(where(~ !all(is.na(.x))))\nelection_results_2020_final |&gt;\n  head(10) |&gt;\n  kable(caption = \"Preview of 2020 Election Results\")\n\n\n\nPreview of 2020 Election Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncounty\ndonald_trump_republican\ndonald_trump_republican_2\njoe_biden_democratic\njoe_biden_democratic_2\nvarious_candidates_other_parties\nvarious_candidates_other_parties_2\nmargin\nmargin_2\ntotal\nstate\ntotal_votes_cast\njoe_biden_dfl\njoe_biden_dfl_2\ncandidate\nfirstalignment\nfirstalignment_2\nfinalalignment_a\nfinalalignment_a_2\ncountyconventiondelegates_b_2\npledgednationalconventiondelegates_c\njoe_biden_democratic_npl\njoe_biden_democratic_npl_2\njo_jorgensen_libertarian\njo_jorgensen_libertarian_2\ndon_blankenship_constitution\nhowie_hawkins_green\nhowie_hawkins_green_2\n\n\n\n\nAutauga\n19838\n71.44\n7503\n27.02\n429\n1.54\n12335\n44.42\n27770\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBaldwin\n83544\n76.17\n24578\n22.41\n1557\n1.42\n58966\n53.76\n109679\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBarbour\n5622\n53.45\n4816\n45.79\n80\n0.76\n806\n7.66\n10518\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBibb\n7525\n78.43\n1986\n20.70\n84\n0.87\n5539\n57.73\n9595\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBlount\n24711\n89.57\n2640\n9.57\n237\n0.86\n22071\n80.00\n27588\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nBullock\n1146\n24.84\n3446\n74.70\n21\n0.46\n-2300\n-49.66\n4613\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nButler\n5458\n57.53\n3965\n41.79\n65\n0.68\n1493\n15.74\n9488\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCalhoun\n35101\n68.85\n15216\n29.85\n666\n1.30\n19885\n39.00\n50983\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nChambers\n8753\n57.27\n6365\n41.64\n166\n1.09\n2388\n15.63\n15284\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nCherokee\n10583\n86.03\n1624\n13.20\n94\n0.77\n8959\n72.83\n12301\nAlabama\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCombining Datasets\nAfter scraping and downloading all our data, we want to combine them to and then do some initial analysis on it.\n\n\nCode\ncounty_shapes &lt;- st_read(\"data/mp04/cb_2023_us_county_500k.shp\") |&gt;\n  mutate(\n    fips = paste0(STATEFP, COUNTYFP),\n    county_clean = tolower(str_replace_all(NAME, \"[^a-z]\", \"\")),\n    state_clean = case_when(\n      STATEFP == \"01\" ~ \"alabama\",\n      STATEFP == \"02\" ~ \"alaska\",\n      STATEFP == \"04\" ~ \"arizona\",\n      STATEFP == \"05\" ~ \"arkansas\",\n      STATEFP == \"06\" ~ \"california\",\n      STATEFP == \"08\" ~ \"colorado\",\n      STATEFP == \"09\" ~ \"connecticut\",\n      STATEFP == \"10\" ~ \"delaware\",\n      STATEFP == \"11\" ~ \"district of columbia\",\n      STATEFP == \"12\" ~ \"florida\",\n      STATEFP == \"13\" ~ \"georgia\",\n      STATEFP == \"15\" ~ \"hawaii\",\n      STATEFP == \"16\" ~ \"idaho\",\n      STATEFP == \"17\" ~ \"illinois\",\n      STATEFP == \"18\" ~ \"indiana\",\n      STATEFP == \"19\" ~ \"iowa\",\n      STATEFP == \"20\" ~ \"kansas\",\n      STATEFP == \"21\" ~ \"kentucky\",\n      STATEFP == \"22\" ~ \"louisiana\",\n      STATEFP == \"23\" ~ \"maine\",\n      STATEFP == \"24\" ~ \"maryland\",\n      STATEFP == \"25\" ~ \"massachusetts\",\n      STATEFP == \"26\" ~ \"michigan\",\n      STATEFP == \"27\" ~ \"minnesota\",\n      STATEFP == \"28\" ~ \"mississippi\",\n      STATEFP == \"29\" ~ \"missouri\",\n      STATEFP == \"30\" ~ \"montana\",\n      STATEFP == \"31\" ~ \"nebraska\",\n      STATEFP == \"32\" ~ \"nevada\",\n      STATEFP == \"33\" ~ \"new hampshire\",\n      STATEFP == \"34\" ~ \"new jersey\",\n      STATEFP == \"35\" ~ \"new mexico\",\n      STATEFP == \"36\" ~ \"new york\",\n      STATEFP == \"37\" ~ \"north carolina\",\n      STATEFP == \"38\" ~ \"north dakota\",\n      STATEFP == \"39\" ~ \"ohio\",\n      STATEFP == \"40\" ~ \"oklahoma\",\n      STATEFP == \"41\" ~ \"oregon\",\n      STATEFP == \"42\" ~ \"pennsylvania\",\n      STATEFP == \"44\" ~ \"rhode island\",\n      STATEFP == \"45\" ~ \"south carolina\",\n      STATEFP == \"46\" ~ \"south dakota\",\n      STATEFP == \"47\" ~ \"tennessee\",\n      STATEFP == \"48\" ~ \"texas\",\n      STATEFP == \"49\" ~ \"utah\",\n      STATEFP == \"50\" ~ \"vermont\",\n      STATEFP == \"51\" ~ \"virginia\",\n      STATEFP == \"53\" ~ \"washington\",\n      STATEFP == \"54\" ~ \"west virginia\",\n      STATEFP == \"55\" ~ \"wisconsin\",\n      STATEFP == \"56\" ~ \"wyoming\",\n      TRUE ~ NA_character_\n    )\n  )\n\n\nReading layer `cb_2023_us_county_500k' from data source \n  `D:\\STA9750-2025-SPRING\\data\\mp04\\data\\mp04\\cb_2023_us_county_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3235 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n\n\nCode\nresults_2020 &lt;- bind_rows(results_2020)\n# Prepare 2020 and 2024 results with clean names\nresults_2020 &lt;- results_2020 |&gt;\n  mutate(\n    county_clean = tolower(str_replace_all(county, \"[^a-z]\", \"\")),\n    state_clean = tolower(state)\n  )\n\nresults_2024 &lt;- election_results_2024_final |&gt;\n  mutate(\n    county_clean = tolower(str_replace_all(county, \"[^a-z]\", \"\")),\n    state_clean = tolower(state)\n  )\n\n# Combine all datasets into one spatial object\nelections_combined &lt;- county_shapes |&gt;\n  left_join(results_2020, by = c(\"state_clean\", \"county_clean\"), suffix = c(\"\", \"_2020\")) |&gt;\n  left_join(results_2024, by = c(\"state_clean\", \"county_clean\"), suffix = c(\"_2020\", \"_2024\"))\n\n# Remove all-NA columns if needed\nelections_combined &lt;- elections_combined |&gt;\n  select(where(~ !all(is.na(.))))\n\nelections_combined &lt;- elections_combined |&gt;\n  mutate(donald_trump_republican_2024 = as.numeric(str_replace_all(donald_trump_republican_2024, \",\", \"\")))\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    joe_biden_democratic = as.numeric(str_replace_all(joe_biden_democratic, \",\", \"\")),\n    total_2020 = as.numeric(str_replace_all(total_2020, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    donald_trump_republican_2020 = as.numeric(str_replace_all(donald_trump_republican_2020, \",\", \"\")),\n    donald_trump_republican_2024 = as.numeric(str_replace_all(donald_trump_republican_2024, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    total_2020 = as.numeric(str_replace_all(total_2020, \",\", \"\")),\n    total_2024 = as.numeric(str_replace_all(total_2024, \",\", \"\"))\n  )\nelections_combined &lt;- elections_combined |&gt;\n  mutate(\n    total_2020 = as.numeric(str_replace_all(as.character(total_2020), \",\", \"\")),\n    total_2024 = as.numeric(str_replace_all(as.character(total_2024), \",\", \"\"))\n  )\n\nView(elections_combined)\n\nelections_combined &lt;- read_csv(\"elections_combined.csv\")\nelections_combined &lt;- elections_combined |&gt;\n  left_join(\n    county_shapes |&gt; \n      st_drop_geometry() |&gt; \n      select(fips, ALAND, AWATER),\n    by = \"fips\"\n  )\nelections_combined &lt;- elections_combined |&gt;\n  janitor::clean_names() |&gt;\n  select(\n    county = name,            \n    statefp = statefp,       \n    state = state_clean,    \n    donald_trump_republican_2020,\n    joe_biden_democratic,\n    total_2020,\n    donald_trump_republican_2024,\n    kamala_harris_democratic,\n    total_2024,\n    geometry,\n    aland,                    \n    awater,\n    fips\n  )\nglimpse(elections_combined)\n\n\nRows: 3,267\nColumns: 13\n$ county                       &lt;chr&gt; \"Baldwin\", \"Houston\", \"Barbour\", \"Sumter\"‚Ä¶\n$ statefp                      &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"05\", \"05\", \"05\",‚Ä¶\n$ state                        &lt;chr&gt; \"alabama\", \"alabama\", \"alabama\", \"alabama‚Ä¶\n$ donald_trump_republican_2020 &lt;dbl&gt; 83544, 32618, 5622, 1598, 11920, 3884, 72‚Ä¶\n$ joe_biden_democratic         &lt;dbl&gt; 24578, 12917, 4816, 4648, 4245, 1116, 455‚Ä¶\n$ total_2020                   &lt;dbl&gt; 109679, 46173, 10518, 6291, 16529, 5202, ‚Ä¶\n$ donald_trump_republican_2024 &lt;dbl&gt; 95798, 32469, 5606, 1542, 11842, 3772, 69‚Ä¶\n$ kamala_harris_democratic     &lt;dbl&gt; 24934, 11352, 4158, 3725, 3769, 862, 3574‚Ä¶\n$ total_2024                   &lt;dbl&gt; 122249, 44349, 9855, 5307, 15803, 4714, 1‚Ä¶\n$ geometry                     &lt;chr&gt; \"list(list(c(-88.02858, -88.023991, -88.0‚Ä¶\n$ aland                        &lt;dbl&gt; 4117725048, 1501742235, 2292160151, 23408‚Ä¶\n$ awater                       &lt;dbl&gt; 1132887203, 4795415, 50523213, 24634880, ‚Ä¶\n$ fips                         &lt;chr&gt; \"01003\", \"01069\", \"01005\", \"01119\", \"0509‚Ä¶"
  },
  {
    "objectID": "data/mp04/mp04.html#initial-analysis",
    "href": "data/mp04/mp04.html#initial-analysis",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nNow, we will do some initial analysis of questions we wanted to answer from the data we gathered.\n1. Which county or counties cast the most votes for Trump (in absolute terms) in 2024?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2024)) |&gt;\n  slice_max(donald_trump_republican_2024, n = 5) |&gt;\n  select(state, county, donald_trump_republican_2024) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Trump Votes in 2024\")\n\n\n\nTop 5 Counties by Trump Votes in 2024\n\n\nstate\ncounty\ndonald_trump_republican_2024\n\n\n\n\ncalifornia\nLos Angeles\n1189862\n\n\ntexas\nHarris\n722695\n\n\ncalifornia\nOrange\n654815\n\n\ncalifornia\nSan Diego\n593270\n\n\ncalifornia\nRiverside\n463677\n\n\n\n\n\n2. Which county or counties cast the most votes for Biden (as a fraction of total votes cast) in 2020?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(joe_biden_democratic), !is.na(total_2020)) |&gt;\n  mutate(biden_pct_2020 = joe_biden_democratic / total_2020) |&gt;\n  slice_max(biden_pct_2020, n = 5) |&gt;\n  select(state, county, biden_pct_2020) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Biden's 2020 Vote Share\")\n\n\n\nTop 5 Counties by Biden‚Äôs 2020 Vote Share\n\n\nstate\ncounty\nbiden_pct_2020\n\n\n\n\ncalifornia\nSan Francisco\n0.8525610\n\n\nmississippi\nJefferson\n0.8513306\n\n\nmississippi\nClaiborne\n0.8478310\n\n\ncalifornia\nMarin\n0.8232561\n\n\nalabama\nMacon\n0.8148573\n\n\n\n\n\n3.County with largest shift towards Trump (absolute increase in votes)\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2020), !is.na(donald_trump_republican_2024)) |&gt;\n  mutate(trump_vote_change = donald_trump_republican_2024 - donald_trump_republican_2020) |&gt;\n  slice_max(trump_vote_change, n = 5) |&gt;\n  select(state, county, trump_vote_change) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Increase in Trump Votes from 2020 to 2024\")\n\n\n\nTop 5 Counties by Increase in Trump Votes from 2020 to 2024\n\n\nstate\ncounty\ntrump_vote_change\n\n\n\n\ncalifornia\nLos Angeles\n44332\n\n\ntexas\nBaylor\n39704\n\n\ntexas\nTaylor\n39704\n\n\ntexas\nBexar\n28927\n\n\ntexas\nMontgomery\n28582\n\n\n\n\n\n4. Which state had the largest shift towards Harris (or smallest shift towards Trump) in 2024? (Note that the total votes for a state can be obtained by summing all counties in that state.)\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(donald_trump_republican_2020), !is.na(donald_trump_republican_2024)) |&gt;\n  group_by(state) |&gt;\n  summarise(\n    trump_2020 = sum(donald_trump_republican_2020, na.rm = TRUE),\n    trump_2024 = sum(donald_trump_republican_2024, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(trump_shift = trump_2024 - trump_2020) |&gt;\n  slice_min(trump_shift, n = 5) |&gt;\n  knitr::kable(caption = \"Top 5 States by Largest Decrease in Trump Votes from 2020 to 2024\")\n\n\n\nTop 5 States by Largest Decrease in Trump Votes from 2020 to 2024\n\n\nstate\ntrump_2020\ntrump_2024\ntrump_shift\n\n\n\n\noregon\n958448\n919480\n-38968\n\n\nindiana\n1729863\n1720347\n-9516\n\n\nmississippi\n756764\n747744\n-9020\n\n\nhawaii\n196865\n193664\n-3201\n\n\narkansas\n760647\n759241\n-1406\n\n\n\n\n\n5. What is the largest county, by area, in this data set?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(aland)) |&gt;\n  slice_max(aland, n = 5) |&gt;\n  select(county, state, aland) |&gt;\n  knitr::kable(caption = \"Top 5 Largest Counties by Area (ALAND)\")\n\n\n\nTop 5 Largest Counties by Area (ALAND)\n\n\ncounty\nstate\naland\n\n\n\n\nYukon-Koyukuk\nalaska\n377055293513\n\n\nNorth Slope\nalaska\n230052121425\n\n\nBethel\nalaska\n105253430580\n\n\nNorthwest Arctic\nalaska\n92367472818\n\n\nSoutheast Fairbanks\nalaska\n64315282486\n\n\n\n\n\n6. Which county has the highest voter density (voters per unit of area) in 2020?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(total_2020), !is.na(aland), aland &gt; 0) |&gt;\n  mutate(voter_density_2020 = total_2020 / aland) |&gt;\n  slice_max(voter_density_2020, n = 5) |&gt;\n  select(county, state, voter_density_2020) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Voter Density in 2020 (Votes per Sq. Meter)\")\n\n\n\nTop 5 Counties by Voter Density in 2020 (Votes per Sq. Meter)\n\n\ncounty\nstate\nvoter_density_2020\n\n\n\n\nSan Francisco\ncalifornia\n0.0036672\n\n\nSt.¬†Louis\nmissouri\n0.0033559\n\n\nOrange\ncalifornia\n0.0007412\n\n\nDallas\ntexas\n0.0004079\n\n\nSt.¬†Louis\nmissouri\n0.0004077\n\n\n\n\n\n7. Which county had the largest increase in voter turnout in 2024?\n\n\nCode\nelections_combined |&gt;\n  filter(!is.na(total_2020), !is.na(total_2024)) |&gt;\n  mutate(turnout_increase = total_2024 - total_2020) |&gt;\n  arrange(desc(turnout_increase)) |&gt;\n  select(county, state, total_2020, total_2024, turnout_increase) |&gt;\n  slice_head(n = 5) |&gt;\n  knitr::kable(caption = \"Top 5 Counties by Increase in Voter Turnout from 2020 to 2024\")\n\n\n\nTop 5 Counties by Increase in Voter Turnout from 2020 to 2024\n\n\ncounty\nstate\ntotal_2020\ntotal_2024\nturnout_increase\n\n\n\n\nBaylor\ntexas\n1702\n55417\n53715\n\n\nTaylor\ntexas\n1702\n55417\n53715\n\n\nWarren\nkentucky\n20064\n56891\n36827\n\n\nBarren\nkentucky\n20064\n56891\n36827\n\n\nMontgomery\ntexas\n271543\n307258\n35715"
  },
  {
    "objectID": "data/mp04/mp04.html#electorial-shift-from-2020-to-2024",
    "href": "data/mp04/mp04.html#electorial-shift-from-2020-to-2024",
    "title": "Exploring US Political Shifts from 2020 to 2024",
    "section": "Electorial Shift from 2020 to 2024",
    "text": "Electorial Shift from 2020 to 2024\nWe created a graph of the United States that how to see the political shift from the 2020 election to the 2024 election.\n\n\nCode\nif (!exists(\"elections_combined\")) {\n  elections_combined &lt;- read_csv(\"data/mp04/elections_combined.csv\")\n}\n\nelection_shifts &lt;- elections_combined |&gt;\n  mutate(\n    trump_pct_2020 = donald_trump_republican_2020 / total_2020,\n    biden_pct_2020 = joe_biden_democratic / total_2020,\n    trump_pct_2024 = donald_trump_republican_2024 / total_2024,\n    harris_pct_2024 = kamala_harris_democratic / total_2024,\n    margin_2020 = trump_pct_2020 - biden_pct_2020,\n    margin_2024 = trump_pct_2024 - harris_pct_2024,\n    margin_shift = margin_2024 - margin_2020,\n    margin_shift_pct = margin_shift * 100\n  ) |&gt;\n  filter(!is.na(margin_shift))\n\n\nif (!exists(\"county_shapes\")) {\n  county_shapes &lt;- st_read(\"data/mp04/cb_2023_us_county_500k.shp\")\n}\n\ncounty_shapes &lt;- county_shapes |&gt;\n  mutate(fips = paste0(STATEFP, COUNTYFP))\n\n\ncounties_with_shifts &lt;- county_shapes |&gt;\n  left_join(election_shifts |&gt;\n              select(fips, margin_shift, margin_shift_pct),\n            by = \"fips\")\n\nstates &lt;- counties_with_shifts |&gt;\n  group_by(STATEFP) |&gt;\n  summarise(geometry = st_union(geometry), .groups = \"drop\") |&gt;\n  st_as_sf()\n\nmove_ak &lt;- function(geom) {\n  geom_scaled &lt;- (geom - st_centroid(geom)) * 0.4  # scale around centroid\n  geom_shifted &lt;- geom_scaled + c(-2500000, -1300000)\n  return(geom_shifted)\n}\n\nmove_hi &lt;- function(geom) {\n  geom_shifted &lt;- geom + c(-1000000, -1400000)\n  return(geom_shifted)\n}\nstates_transformed &lt;- states |&gt;\n  st_transform(5070)\n\nstates_transformed$geometry[states_transformed$STATEFP == \"02\"] &lt;- move_ak(states_transformed$geometry[states_transformed$STATEFP == \"02\"])\nstates_transformed$geometry[states_transformed$STATEFP == \"15\"] &lt;- move_hi(states_transformed$geometry[states_transformed$STATEFP == \"15\"])\n\ncounties_transformed &lt;- counties_with_shifts |&gt;\n  st_transform(5070)\n\ncounties_transformed$geometry[counties_transformed$STATEFP == \"02\"] &lt;- move_ak(counties_transformed$geometry[counties_transformed$STATEFP == \"02\"])\ncounties_transformed$geometry[counties_transformed$STATEFP == \"15\"] &lt;- move_hi(counties_transformed$geometry[counties_transformed$STATEFP == \"15\"])\narrow_scale &lt;- 20000\n\ncounties_arrows &lt;- counties_transformed |&gt;\n  filter(!is.na(margin_shift)) |&gt;\n  mutate(\n    centroid = st_centroid(geometry),\n    x = st_coordinates(centroid)[, 1],\n    y = st_coordinates(centroid)[, 2],\n    angle = if_else(margin_shift &gt; 0, 0, pi),\n    length = abs(margin_shift) * arrow_scale,\n    x_end = x + cos(angle) * length,\n    y_end = y + sin(angle) * length\n  )\nggplot() +\n  geom_sf(data = states_transformed, fill = \"white\", color = \"gray70\", size = 0.2) +\n  geom_segment(\n    data = counties_arrows,\n    aes(x = x, y = y, xend = x_end, yend = y_end, color = margin_shift &gt; 0),\n    arrow = arrow(length = unit(0.1, \"cm\")),\n    size = 0.3,\n    alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"blue\"),\n    labels = c(\"TRUE\" = \"More Rep.\", \"FALSE\" = \"More Dem.\"),\n    name = \"Shift from 2020 to 2024\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"County-Level Shift in Vote Margin: 2020 ‚Üí 2024\",\n    caption = \"Data: Wikipedia county election results\"\n  ) +\n  coord_sf(\n    crs = st_crs(counties_transformed),\n    xlim = c(-2500000, 2500000),  # tweak these numbers\n    ylim = c(-1000000, 3500000),  # tweak these numbers\n    expand = FALSE\n  )\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"nyt_county_shift_map_large.png\", width = 18, height = 11, dpi = 300)"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "",
    "text": "This white paper is for the analysis of New York City‚Äôs payroll data and policies that the Commission to Analyze Taxpayer Spending (CATS) have created with recommendations on how to optimize tax payer‚Äôs money more efficiently. We will be looking through two policies from CATS, analyzing if they are policies that should be adopted or not, and one additional suggested policy for CATS to adopt that will reduce expenses and city payroll. The policies we will be looking at are as follows:\n\nCapping Salaries at Mayoral Level: This proposal from CATS is a traditional policy that is implementented in many other governments already but not yet in the city of New York. Implimenting this policy could potentially save the city money but can lead to more problems down the line.\nIncreasing Staffing to Reduce Overtime Expenses: This policy suggests hiring additional employees to reduce excessive overtime pay. There are many employees that work many overtime hours which leads to higher payroll costs and by hiring more employees,and overall it could reduce overtime expenses.\nAdjusting Pay Based on Geographic Location: Our suggestion to CATS to effectively save city payroll is to adjust salaries based on the work location borough to reflect on living costs where some may have a higher cost of living while others have lower."
  },
  {
    "objectID": "mp01.html#purpose",
    "href": "mp01.html#purpose",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "",
    "text": "This white paper is for the analysis of New York City‚Äôs payroll data and policies that the Commission to Analyze Taxpayer Spending (CATS) have created with recommendations on how to optimize tax payer‚Äôs money more efficiently. We will be looking through two policies from CATS, analyzing if they are policies that should be adopted or not, and one additional suggested policy for CATS to adopt that will reduce expenses and city payroll. The policies we will be looking at are as follows:\n\nCapping Salaries at Mayoral Level: This proposal from CATS is a traditional policy that is implementented in many other governments already but not yet in the city of New York. Implimenting this policy could potentially save the city money but can lead to more problems down the line.\nIncreasing Staffing to Reduce Overtime Expenses: This policy suggests hiring additional employees to reduce excessive overtime pay. There are many employees that work many overtime hours which leads to higher payroll costs and by hiring more employees,and overall it could reduce overtime expenses.\nAdjusting Pay Based on Geographic Location: Our suggestion to CATS to effectively save city payroll is to adjust salaries based on the work location borough to reflect on living costs where some may have a higher cost of living while others have lower."
  },
  {
    "objectID": "mp01.html#objective",
    "href": "mp01.html#objective",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "Objective",
    "text": "Objective\nThe objective for our analysis is to see how these possible policies will affect overall spending and we will do that by computing each policies impact on city payroll and determing if any staffing adjustments are required to implement the policy."
  },
  {
    "objectID": "mp01.html#initial-data",
    "href": "mp01.html#initial-data",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "Initial Data",
    "text": "Initial Data\nFor the analysis, we are using the NYC Payroll Data. We first want to look at all the data provided and standardize the data to usE and see what is applicable to the questions we want to answer, and from the dataset we are able to view an example of the different columns provided. Some key variables that we want to focus on include but are not limited to fiscal_year, agency_name, title description, base_salary, total_ot_paid, regular_hours, and ot_hours.\n\n\nCode\n# calcuating how much payroll increases per year\npayroll_growth &lt;- df |&gt; \n  group_by(fiscal_year) |&gt; \n  summarize(total_payroll = sum(base_salary + total_ot_paid, na.rm = TRUE)) |&gt; \n  arrange(fiscal_year) |&gt; \n  mutate(percentage_increase = (total_payroll - lag(total_payroll)) / lag(total_payroll) * 100) |&gt; \n  drop_na(percentage_increase)  \n#plotting of the data\nggplot(payroll_growth, aes(x = fiscal_year, y = percentage_increase)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +  \n  geom_point(color = \"steelblue\", size = 2) +\n  labs(title = \"Year-over-Year Percentage Increase in Aggregate Payroll\",\n       x = \"Fiscal Year\",\n       y = \"Percentage Increase (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nHere we can see the changes in aggregated payroll per year, with 2015 having the biggest increase by 13% and it was common for the payroll spending to increase slowly every year. If this trend were to continue there would be too much payroll spending and the cost needs to be controlled.\n\nStandardization\n\n\nCode\n#standardizing data\ndf &lt;- df |&gt;\n  mutate(\n    `agency_name` = str_to_title(`agency_name`),\n    `last_name` = str_to_title(`last_name`),\n    `first_name` = str_to_title(`first_name`),\n    `work_location_borough` = str_to_title(`work_location_borough`),\n    `title_description` = str_to_title(`title_description`),\n    `leave_status` = str_to_title(`leave_status_as_of_june_30`)\n  )\n\n\nWe standardized the data by converting all the strings using the ‚Äústr_to_title‚Äù function so only the first letter of the word is capitalized to have a more concise data table.\n\n\nCode\n#small dataset showing a sample of payroll data\ndf |&gt;\n  head(5)|&gt;\n  kable(caption = \"Sample of Payroll Data\")\n\n\n\nSample of Payroll Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiscal_year\npayroll_number\nagency_name\nlast_name\nfirst_name\nmid_init\nagency_start_date\nwork_location_borough\ntitle_description\nleave_status_as_of_june_30\nbase_salary\npay_basis\nregular_hours\nregular_gross_paid\not_hours\ntotal_ot_paid\ntotal_other_pay\nleave_status\n\n\n\n\n2024\n67\nAdmin For Children‚Äôs Svcs\nFaye Fall\nSokhna\nM\n2023-11-20\nBronx\nChild Protective Specialist\nACTIVE\n62043\nper Annum\n1050.00\n31267.96\n12.00\n425.00\n78.04\nActive\n\n\n2024\n67\nAdmin For Children‚Äôs Svcs\nKilgore\nOrlantha\nB\n2023-08-28\nBrooklyn\nChild Protective Specialist\nACTIVE\n62043\nper Annum\n1470.00\n44660.96\n99.75\n3859.84\n78.14\nActive\n\n\n2024\n67\nAdmin For Children‚Äôs Svcs\nWisdom\nCherise\nM\n2022-10-24\nManhattan\nCommunity Associate\nON LEAVE\n43144\nper Annum\n1251.50\n28649.20\n30.00\n802.42\n78.26\nOn Leave\n\n\n2024\n67\nAdmin For Children‚Äôs Svcs\nMiller\nMoya-Gaye\nS\n2023-02-27\nManhattan\nChild Protective Specialist\nON LEAVE\n62043\nper Annum\n1400.75\n44515.43\n44.75\n1476.98\n78.37\nOn Leave\n\n\n2024\n67\nAdmin For Children‚Äôs Svcs\nBradley\nYashika\nM\n2023-02-27\nBronx\nChild Protective Specialist\nCEASED\n60236\nper Annum\n700.00\n22133.64\n53.00\n1933.33\n78.47\nCeased\n\n\n\n\n\nNext, we will start looking at the policies and some key findings and information that CATS wanted us to provide for analysis."
  },
  {
    "objectID": "mp01.html#policy-1-capping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-1-capping-salaries-at-mayoral-level",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "Policy 1: Capping Salaries at Mayoral Level ",
    "text": "Policy 1: Capping Salaries at Mayoral Level \n\n\nCode\n#calculating Mayor Eric Adam's salary and position each year\ndf_mayor &lt;- df |&gt; \n  filter(first_name == 'Eric', last_name == 'Adams', mid_init == 'L') |&gt;\n  select(fiscal_year, title_description, agency_name, base_salary) |&gt;\n  rename('Fiscal Year' = fiscal_year,\n         'Position' = title_description,\n         'Agency' = agency_name,\n         'Total Salary' = base_salary) |&gt;\n  group_by(`Fiscal Year`) |&gt;  \n  summarize(\n    `Total Salary` = sum(`Total Salary`),\n    `Position` = paste(unique(`Position`), collapse = \" / \"),  \n    `Agency` = paste(unique(`Agency`), collapse = \" / \")  \n  ) |&gt;  \n  arrange(`Fiscal Year`) |&gt;\n  kable(caption = \"Mayor's Career Path and Salary\", \n        format = \"pipe\")\n\ndf_mayor\n\n\n\nMayor‚Äôs Career Path and Salary\n\n\n\n\n\n\n\n\nFiscal Year\nTotal Salary\nPosition\nAgency\n\n\n\n\n2014\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2015\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2016\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2017\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2018\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2019\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2020\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2021\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2022\n437950\nBorough President / Mayor\nBorough President-Brooklyn / Office Of The Mayor\n\n\n2023\n258750\nMayor\nOffice Of The Mayor\n\n\n2024\n258750\nMayor\nOffice Of The Mayor\n\n\n\n\n\nAs you can see from the data, there was one anomaly where Mayor Eric Adams transitioned from Borough President to Mayor of New York, instead of seperating each of the salaries we decided to put them together although he may have not earned that amount total. To make it simpler to calculate everything.\n\n\nCode\n#Calcuating mayor salary\nmayor_salaries &lt;- df |&gt; \n  filter(title_description == \"Mayor\") |&gt; \n  group_by(fiscal_year) |&gt; \n  summarize(base_salary_mayor = max(base_salary, na.rm = TRUE))\n#combining dataset to see which jobs have a base salary that is higher than the base of the mayor\ndf_updated &lt;- df |&gt; \n  left_join(mayor_salaries, by = \"fiscal_year\") |&gt; \n  mutate(above_mayor_salary = base_salary &gt; base_salary_mayor)\n#filtering to see the top 10 employees that make more than the mayor\nemployees_above_mayor &lt;- df_updated |&gt; \n  filter(above_mayor_salary) |&gt; \n  select(fiscal_year, agency_name, title_description, base_salary, base_salary_mayor)\nkable(head(employees_above_mayor, 10), caption = \"Top 10 Employees Earning More Than the Mayor\")\n\n\n\nTop 10 Employees Earning More Than the Mayor\n\n\n\n\n\n\n\n\n\nfiscal_year\nagency_name\ntitle_description\nbase_salary\nbase_salary_mayor\n\n\n\n\n2024\nAdmin For Children‚Äôs Svcs\nDeputy Commissioner\n258866\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nAdministrative Staff Analyst\n264748\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nAssistant Commissioner For Facilities Development & Const\n264746\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nExecutive Agency Counsel\n264746\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nAdministrative Director Of Social Services\n264835\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nCity Medical Director\n264062\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nCommissioner Of Children‚Äôs Services\n277605\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nStrategic Initiative Specialist\n264746\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nExecutive Agency Counsel\n264835\n258750\n\n\n2024\nAdmin For Children‚Äôs Svcs\nAdministrative Director Of Social Services\n271810\n258750\n\n\n\n\n\nCode\n#calculating amount of agencies that would be affected by the change\nimpacted_agencies &lt;- employees_above_mayor |&gt;  \n  count(agency_name, sort = TRUE) \n#calculating which roles would get impacted by the change\nimpacted_titles &lt;- employees_above_mayor |&gt;  \n  count(title_description, sort = TRUE)  \nkable(slice_head(impacted_agencies, n = 10), caption = \"Top 10 Agencies with Employees Earning More Than the Mayor\")\n\n\n\nTop 10 Agencies with Employees Earning More Than the Mayor\n\n\nagency_name\nn\n\n\n\n\nOffice Of The Comptroller\n65\n\n\nPolice Department\n47\n\n\nOffice Of The Mayor\n44\n\n\nFire Department\n32\n\n\nDept Of Environment Protection\n19\n\n\nAdmin For Children‚Äôs Svcs\n15\n\n\nDept Of Ed Pedagogical\n15\n\n\nOffice Of The Actuary\n15\n\n\nTechnology & Innovation\n15\n\n\nDepartment Of Sanitation\n13\n\n\n\n\n\nCode\nkable(slice_head(impacted_titles, n = 10), caption = \"Top 10 Job Titles with Employees Earning More Than the Mayor\")\n\n\n\nTop 10 Job Titles with Employees Earning More Than the Mayor\n\n\ntitle_description\nn\n\n\n\n\nAdministrative Staff Analyst\n33\n\n\nPresident\n33\n\n\nDirector Of Investments\n27\n\n\nDeputy Commissioner\n25\n\n\nExecutive Director\n18\n\n\nExecutive Agency Counsel\n16\n\n\nComputer Systems Manager\n14\n\n\nFirst Deputy Mayor\n14\n\n\nChief Actuary\n13\n\n\nDeputy Assistant Chief Of Department\n13\n\n\n\n\n\nCode\n#seeing the total amount that would be saved if the policy were to go through\nemployees_above_mayor &lt;- employees_above_mayor |&gt;  \n  mutate(savings = base_salary - base_salary_mayor)  \ntotal_savings &lt;- sum(employees_above_mayor$savings, na.rm = TRUE)\ntotal_people_above_mayor &lt;- nrow(employees_above_mayor)\ncat(\"**Total Savings:**\", total_savings, \"\\n\\n\")\n\n\n**Total Savings:** 8958853 \n\n\nCode\ncat(\"**Total People Above Mayor's Salary:**\", total_people_above_mayor)\n\n\n**Total People Above Mayor's Salary:** 451\n\n\nFor the analysis, we only focused on the base salaries of each role to compare them all to the mayors to see how many people and jobs would be affected by it. We also researched to see the top 10 employees, agencies, and titles affected by the change. The agencies that would be most affected by the policy change would be the Office of the Comptroller, Police Department, Office of the Mayor, and Fire Department and would directly affect over 400 people. According to our calculations, if the policy were to go through the total amount saved throughout all the years documented in the dataset would be $8,958,853, and it would affect 451 jobs.\n\nPolicy 1 Recommendation:\nWe would recommend for CATS to not go through with this policy because although it would significantly reduce payroll costs and prevent excessive overtime spending, it is not worth the risk of employee dissatisfaction which may lead to people quitting their jobs for better paying private sector jobs. Public safety agencies such as the NYPD or Fire Department may need the overtime pay to function effectively so limiting their compensation could lead to staff shortages. Instead of completely capping salaries at mayoral level, there can be a compromise where non-essential workers pay can be capped at the mayoral level while critial roles do not have that cap applied to them."
  },
  {
    "objectID": "mp01.html#policy-2-increasing-staffing-to-reduce-overtime-expenses",
    "href": "mp01.html#policy-2-increasing-staffing-to-reduce-overtime-expenses",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "Policy 2: Increasing Staffing to Reduce Overtime Expenses ",
    "text": "Policy 2: Increasing Staffing to Reduce Overtime Expenses \nWe want to find out how many more Full Time Employees (fte in the tables) would be neccesary for overtime expenses to be completely cut to be able to see the scale of the amount of overtime hours worked by employees although it will probably be unlikely that the required amount of new Full Time Employees are fulfilled.\n\n\nCode\n#grouping to find total overtime pay, hours, and average rate of jobs\novertime_analysis &lt;- df |&gt; \n  group_by(agency_name, title_description) |&gt; \n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    total_overtime_pay = sum(total_ot_paid, na.rm = TRUE),\n    average_hourly_rate = mean(base_salary / 2080, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    required_fte = ceiling(total_overtime_hours / 2080),\n    overtime_cost = total_overtime_hours * average_hourly_rate * 1.5, \n    regular_cost = total_overtime_hours * average_hourly_rate,         \n    potential_savings = overtime_cost - regular_cost\n  )\n#finding the most jobs with total overtime hours and the required amount of new full time employees for the policy to work\novertime_fte_analysis &lt;- df |&gt; \n  group_by(agency_name, title_description) |&gt; \n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    required_fte = ceiling(total_overtime_hours / 2080)\n  ) |&gt; \n  arrange(desc(total_overtime_hours))\nkable(head(overtime_fte_analysis, 10), caption = \"Top 10 Job Titles by Overtime FTE Analysis\")\n\n\n\nTop 10 Job Titles by Overtime FTE Analysis\n\n\n\n\n\n\n\n\nagency_name\ntitle_description\ntotal_overtime_hours\nrequired_fte\n\n\n\n\nPolice Department\nPolice Officer\n60270018\n28976\n\n\nFire Department\nFirefighter\n43536213\n20931\n\n\nDepartment Of Correction\nCorrection Officer\n34092745\n16391\n\n\nDepartment Of Sanitation\nSanitation Worker\n23098266\n11105\n\n\nPolice Department\nP.o. Da Det Gr3\n15622342\n7511\n\n\nPolice Department\nSchool Safety Agent\n14982551\n7204\n\n\nNyc Housing Authority\nCaretaker\n9439229\n4539\n\n\nPolice Department\nSergeant-\n8251354\n3967\n\n\nFire Department\nLieutenant\n7607797\n3658\n\n\nPolice Department\nTraffic Enforcement Agent\n6913560\n3324\n\n\n\n\n\nCode\n#calcuating potential savings from each department and how many people would be needed\nagency_savings &lt;- overtime_analysis |&gt; \n  group_by(agency_name) |&gt; \n  summarize(\n    total_potential_savings = sum(potential_savings, na.rm = TRUE),\n    total_required_fte = sum(required_fte, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt; \n  arrange(desc(total_potential_savings))\nkable(head(agency_savings, 10), caption = \"Top 10 Agencies by Potential Savings\")\n\n\n\nTop 10 Agencies by Potential Savings\n\n\n\n\n\n\n\nagency_name\ntotal_potential_savings\ntotal_required_fte\n\n\n\n\nPolice Department\n2589731595\n68333\n\n\nFire Department\n1333482940\n34860\n\n\nDepartment Of Correction\n741593209\n20810\n\n\nDepartment Of Sanitation\n535014906\n15232\n\n\nNyc Housing Authority\n172575977\n12698\n\n\nHra/Dept Of Social Services\n171591017\n6721\n\n\nAdmin For Children‚Äôs Svcs\n148044480\n5113\n\n\nDepartment Of Transportation\n126130646\n6287\n\n\nDept Of Environment Protection\n78524051\n4462\n\n\nDept. Of Homeless Services\n51569931\n2340\n\n\n\n\n\nThe main focus of policy 2 is on staffing and overtime hours, so we analyzed which agencies and roles have the most overtime, unsurprisingly it is the NYPD and Fire Department because their jobs require them to be prepared to work at any moment. Although the potential savings are high, it would be difficult for every missing job to be filled.\nPolicy 2 Recommendation\nWe believe that this policy is not very feasible because of the sheer number new full time employees are needed for certain jobs. Just Police Officers themselves worked over 60,000,000 hours of overtime, and that would require almost 30,000 new full time Police Officers if they wanted to cut overtime expenses totally out from that title. We would recommend limiting the amount of overtime hours an employee can have, with a maximum of 10% of their regular hours. Excess hours will not be paid as overtime but as regular hours. Another suggestion is increasing staffing in smaller agencies and roles first and continually monitor it to see how much payroll is saved and scale it if it is successful."
  },
  {
    "objectID": "mp01.html#policy-3-adjusting-pay-based-on-geographic-location",
    "href": "mp01.html#policy-3-adjusting-pay-based-on-geographic-location",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": "Policy 3: Adjusting Pay Based on Geographic Location ",
    "text": "Policy 3: Adjusting Pay Based on Geographic Location \nTo analyze our suggestion to CATS, we looked at only 4 boroughs which were Manhattan, Brooklyn, Queens, and Bronx because the dataset did not have Staten Island in it. Through our analysis we are comparing salaries across boroughs and estimating savings by reducing salaries in lower-cost areas.\n\n\nCode\n#consolidating data to only the 4 boroughs\nnyc_boroughs &lt;- c(\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\")\nnyc_data &lt;- df |&gt; \n  filter(work_location_borough %in% nyc_boroughs)\n#finding the average salary per borough\nborough_salary_nyc &lt;- nyc_data |&gt; \n  group_by(work_location_borough) |&gt; \n  summarize(avg_base_salary = mean(base_salary, na.rm = TRUE), .groups = 'drop')\nkable(borough_salary_nyc, caption = \"Average Base Salary for NYC Boroughs\")\n\n\n\nAverage Base Salary for NYC Boroughs\n\n\nwork_location_borough\navg_base_salary\n\n\n\n\nBronx\n51598.13\n\n\nBrooklyn\n55627.04\n\n\nManhattan\n40990.79\n\n\nQueens\n54643.50\n\n\n\n\n\nCode\n#finding lowest cost borough\nlowest_cost_borough &lt;- borough_salary_nyc |&gt; \n  filter(avg_base_salary == min(avg_base_salary)) |&gt; \n  pull(work_location_borough)\nprint(paste(\"Lowest-Cost Borough:\", lowest_cost_borough))\n\n\n[1] \"Lowest-Cost Borough: Manhattan\"\n\n\nCode\n#adjusteding salaries by lowering it by 10% if it was not in the lowest cost borough\nadjusted_salaries &lt;- nyc_data |&gt; \n  left_join(borough_salary_nyc, by = \"work_location_borough\") |&gt; \n  mutate(\n    adjusted_salary = ifelse(\n      work_location_borough != lowest_cost_borough,\n      base_salary * 0.9,  \n      base_salary\n    ),\n#calculating salary reductions\n    salary_reduction = pmax(0, base_salary - adjusted_salary)\n  )\n#calculating total savings from the reductions\ntotal_savings &lt;- sum(adjusted_salaries$salary_reduction, na.rm = TRUE)\npaste(\"Total Potential Savings by Adjusting Salaries:\", dollar(total_savings))\n\n\n[1] \"Total Potential Savings by Adjusting Salaries: $7,793,789,406\"\n\n\nCode\nsavings_by_borough &lt;- adjusted_salaries |&gt; \n  group_by(work_location_borough) |&gt; \n  summarize(total_savings = sum(salary_reduction, na.rm = TRUE), .groups = 'drop') |&gt; \n  arrange(desc(total_savings))\nkable(savings_by_borough, caption = \"Potential Savings by Borough\")\n\n\n\nPotential Savings by Borough\n\n\nwork_location_borough\ntotal_savings\n\n\n\n\nQueens\n3398011415\n\n\nBrooklyn\n2916892892\n\n\nBronx\n1478885099\n\n\nManhattan\n0\n\n\n\n\n\nPolicy 3 Recommendation\nAs we can see from the analysis, since Manhattan is the lowest cost borough by a massive amount so they do not get a reduction in salary while the other 3 boroughs get a reduction. We decided that the reduction would be 10% of their base salaries and it would be almost $8,000,000 in potential savings if the policy were to go through. This change would be beneficial for CATS as they are looking to cut payroll costs."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "New York City Payroll Policy Proposal Analysis",
    "section": " Conclusion ",
    "text": "Conclusion \nOverall based on our analysis of the payroll data, New York City‚Äôs payroll policies need some sort of reform to cut costs but also needs to be balanced to keep employee retention high. Capping salaries at the mayoral level may have the potential of losing highly skilled employees in their respective fields. Increasing staff is difficult to do when overtime plays a big role in some of these jobs and it is not possible to fill in all the overtime hours with new full time employees. Adjusting pay based on the borough allows for fair compensation aligning with different costs of living. To cut down on payroll spending and optimize the taxpayer‚Äôs money we recommend a hybrid combination of the recommendations to maintain financial sustanability in New York city."
  },
  {
    "objectID": "mp03.html#creating-the-ultimate-playlist",
    "href": "mp03.html#creating-the-ultimate-playlist",
    "title": "The Loverboy Playlist",
    "section": "Creating the Ultimate Playlist",
    "text": "Creating the Ultimate Playlist\nThe Loverboy Playlist is 12 carefully curated tracks meant to represent the journey of love and heartbreak, capturing the highs and lows of it all. Whether you are mending a broken heart or yearning for that feeling of love, this playlist will allow you to let all your emotions out and find comfort in knowing that everything will be okay in the end."
  },
  {
    "objectID": "mp03.html#design-principles",
    "href": "mp03.html#design-principles",
    "title": "The Loverboy Playlist",
    "section": "Design Principles",
    "text": "Design Principles\nHeuristics: I decided to use my favorite song as the anchor for this analysis, it being The Way Life Goes (feat. Oh Wonder) by Lil Uzi Vert. The songs were selected using basic heuristics such as shared playlist appearances, similar keys and tempos, same artist, same year with similar audio, and similar valence.\n\nNarrative: Every song contributes to the emotional theme of love, loss, and growth. As the energy of the songs ramp up, so does the emotions, inviting listeners to feel vulnerable and accept all the good and bad that comes with being in love.\n\nHidden Gems: Among the 12 tracks, at least 5 are considered ‚Äúnot popular‚Äù (Spotify Popularity &lt; 70), with hopes to highlight lesser known tracks. Through analysis I have also discovered 2 new tracks that I added to the playlist that conveyed the similar feelings."
  },
  {
    "objectID": "mp03.html#data-acquisition-and-cleaning",
    "href": "mp03.html#data-acquisition-and-cleaning",
    "title": "The Loverboy Playlist",
    "section": "Data Acquisition and cleaning",
    "text": "Data Acquisition and cleaning\nDownloading Song Characteristic Data\n\n\nCode\nload_songs &lt;- function() {\n  url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  dir_path &lt;- \"data/mp03\"\n  file_path &lt;- file.path(dir_path, \"spotify_data.csv\")\n  \n\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n\n  if (!file.exists(file_path)) {\n    message(\"Downloading Spotify song dataset...\")\n    download.file(url, destfile = file_path, mode = \"wb\")\n  } else {\n    message(\"Using cached file.\")\n  }\n  \n\n  songs_raw &lt;- readr::read_csv(file_path, show_col_types = FALSE)\n  \n\n  clean_artist_string &lt;- function(x){\n    x |&gt;\n      stringr::str_replace_all(\"\\\\['\", \"\") |&gt;\n      stringr::str_replace_all(\"'\\\\]\", \"\") |&gt;\n      stringr::str_replace_all(\"[ ]?'\", \"\") |&gt;\n      stringr::str_replace_all(\"[ ]*,[ ]*\", \",\")\n  }\n  songs_clean &lt;- songs_raw |&gt;\n    tidyr::separate_longer_delim(artists, \",\") |&gt;\n    dplyr::mutate(artist = clean_artist_string(artists)) |&gt;\n    dplyr::select(-artists)\n  \n  return(songs_clean)\n}\nSONGS &lt;- load_songs()\n\n\nUsing cached file.\n\n\nCode\nSONGS |&gt;\n  select(name, artist, year, popularity, danceability, energy) |&gt;\n  slice_head(n = 10) |&gt;\n  knitr::kable(caption = \"Sample of Cleaned Spotify Songs Data\")\n\n\n\nSample of Cleaned Spotify Songs Data\n\n\n\n\n\n\n\n\n\n\nname\nartist\nyear\npopularity\ndanceability\nenergy\n\n\n\n\nSingende Bataillone 1. Teil\nCarl Woitschach\n1928\n0\n0.708\n0.1950\n\n\nFantasiest√ºcke, Op. 111: Pi√π tosto lento\nRobert Schumann\n1928\n0\n0.379\n0.0135\n\n\nFantasiest√ºcke, Op. 111: Pi√π tosto lento\nVladimir Horowitz\n1928\n0\n0.379\n0.0135\n\n\nChapter 1.18 - Zamek kaniowski\nSeweryn Goszczy≈Ñski\n1928\n0\n0.749\n0.2200\n\n\nBebamos Juntos - Instrumental (Remasterizado)\nFrancisco Canaro\n1928\n0\n0.781\n0.1300\n\n\nPolonaise-Fantaisie in A-Flat Major, Op. 61\nFr√©d√©ric Chopin\n1928\n1\n0.210\n0.2040\n\n\nPolonaise-Fantaisie in A-Flat Major, Op. 61\nVladimir Horowitz\n1928\n1\n0.210\n0.2040\n\n\nScherzo a capriccio: Presto\nFelix Mendelssohn\n1928\n0\n0.424\n0.1200\n\n\nScherzo a capriccio: Presto\nVladimir Horowitz\n1928\n0\n0.424\n0.1200\n\n\nValse oubli√©e No.¬†1 in F-Sharp Major, S. 215/1\nFranz Liszt\n1928\n0\n0.444\n0.1970\n\n\n\n\n\nDownloading Playlists Data1\n\n\nCode\nload_playlists &lt;- function(start = 0, stop = 300000, step = 1000) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  local_dir &lt;- \"data/mp03/playlists\"\n  \n  if (!dir.exists(local_dir)) {\n    dir.create(local_dir, recursive = TRUE)\n  }\n  \n  playlist_list &lt;- list()\n  i &lt;- 1\n  \n  for (start_val in seq(start, stop, by = step)) {\n    end_val &lt;- start_val + step - 1\n    filename &lt;- sprintf(\"mpd.slice.%d-%d.json\", start_val, end_val)\n    file_path &lt;- file.path(local_dir, filename)\n    file_url &lt;- paste0(base_url, filename)\n    file_found &lt;- TRUE\n    \n    if (!file.exists(file_path)) {\n      message(\"Attempting download: \", filename)\n      tryCatch({\n        download.file(file_url, destfile = file_path, mode = \"wb\")\n        message(\"Downloaded: \", filename)\n      }, error = function(e) {\n        message(\"Skipped (not found): \", filename)\n        file_found &lt;&lt;- FALSE\n      })\n    } else {\n      message(\"Using cached: \", filename)\n    }\n    if (file_found && file.exists(file_path)) {\n      json &lt;- jsonlite::fromJSON(file_path, flatten = TRUE)\n      playlist_list[[i]] &lt;- json$playlists\n      i &lt;- i + 1\n    }\n  }\n  \n  return(dplyr::bind_rows(playlist_list))\n}\n#reducing the number of files to be processed because there are too many files. approved by professor for performance reasons\nplaylists &lt;- load_playlists(start = 0, stop = 10000, step = 1000)\nplaylists |&gt;\n  select(name, num_tracks, num_followers, duration_ms) |&gt;\n  slice_head(n = 10) |&gt;\n  knitr::kable(caption = \"Sample of Loaded Playlist Metadata\")\n\n\n\nSample of Loaded Playlist Metadata\n\n\nname\nnum_tracks\nnum_followers\nduration_ms\n\n\n\n\nThrowbacks\n52\n1\n11532414\n\n\nAwesome Playlist\n39\n1\n11656470\n\n\nkorean\n64\n1\n14039958\n\n\nmat\n126\n1\n28926058\n\n\n90s\n17\n2\n4335282\n\n\nWedding\n80\n1\n19156557\n\n\nI Put A Spell On You\n16\n1\n3408479\n\n\n2017\n53\n1\n12674796\n\n\nBOP\n46\n2\n9948921\n\n\nold country\n21\n1\n4297488\n\n\n\n\n\nCleaning and Formatting Playlist Data\n\n\nCode\nrectangle_playlists &lt;- function(playlist_df) {\n  strip_spotify_prefix &lt;- function(x){\n    stringr::str_extract(x, \".*:.*:(.*)\", group=1)\n  }\n  \n  playlist_df |&gt;\n    tidyr::unnest(cols = tracks, names_sep = \"_\") |&gt;\n    dplyr::mutate(\n      playlist_position  = dplyr::row_number(),\n      track_id           = strip_spotify_prefix(tracks_track_uri),\n      artist_id          = strip_spotify_prefix(tracks_artist_uri),\n      album_id           = strip_spotify_prefix(tracks_album_uri)\n    ) |&gt;\n    dplyr::transmute(\n      playlist_name      = name,\n      playlist_id        = pid,\n      playlist_position,\n      playlist_followers = num_followers,\n      artist_name        = tracks_artist_name,\n      artist_id,\n      track_name         = tracks_track_name,\n      track_id,\n      album_name         = tracks_album_name,\n      album_id,\n      duration           = tracks_duration_ms\n    )\n}\nplaylist_metadata &lt;- load_playlists(start = 0, stop = 10000, step = 1000)\n\nrectangular_df &lt;- rectangle_playlists(playlist_metadata)\n\nplaylist_metadata &lt;- load_playlists(start = 0, stop = 1999, step = 1000)\nstr(playlist_metadata[[1]])\n\n\n chr [1:2000] \"Throwbacks\" \"Awesome Playlist\" \"korean \" \"mat\" \"90s\" ...\n\n\nCode\nrectangular_df &lt;- rectangle_playlists(playlist_metadata)\nplaylist_df &lt;- bind_rows(playlist_metadata)\ntrack_df &lt;- tidyr::unnest(playlist_df, cols = tracks, names_sep = \"_\")\nrectangular_df |&gt;\n  select(\n    playlist_name,\n    track_name,\n    artist_name,\n    album_name,\n    duration,\n    playlist_followers\n  ) |&gt;\n  slice_head(n = 10) |&gt;\n  knitr::kable(caption = \"Sample of Cleaned Playlist-Track Data\")\n\n\n\nSample of Cleaned Playlist-Track Data\n\n\n\n\n\n\n\n\n\n\nplaylist_name\ntrack_name\nartist_name\nalbum_name\nduration\nplaylist_followers\n\n\n\n\nThrowbacks\nLose Control (feat. Ciara & Fat Man Scoop)\nMissy Elliott\nThe Cookbook\n226863\n1\n\n\nThrowbacks\nToxic\nBritney Spears\nIn The Zone\n198800\n1\n\n\nThrowbacks\nCrazy In Love\nBeyonc√©\nDangerously In Love (Alben f√ºr die Ewigkeit)\n235933\n1\n\n\nThrowbacks\nRock Your Body\nJustin Timberlake\nJustified\n267266\n1\n\n\nThrowbacks\nIt Wasn‚Äôt Me\nShaggy\nHot Shot\n227600\n1\n\n\nThrowbacks\nYeah!\nUsher\nConfessions\n250373\n1\n\n\nThrowbacks\nMy Boo\nUsher\nConfessions\n223440\n1\n\n\nThrowbacks\nButtons\nThe Pussycat Dolls\nPCD\n225560\n1\n\n\nThrowbacks\nSay My Name\nDestiny‚Äôs Child\nThe Writing‚Äôs On The Wall\n271333\n1\n\n\nThrowbacks\nHey Ya! - Radio Mix / Club Mix\nOutKast\nSpeakerboxxx/The Love Below\n235213\n1"
  },
  {
    "objectID": "mp03.html#initial-analysis",
    "href": "mp03.html#initial-analysis",
    "title": "The Loverboy Playlist",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nWe first had some initial exploration we wanted to do with the playlist dataset.\n\n\n\nCode\nrectangular_df |&gt;\n  dplyr::summarise(\n    unique_tracks = n_distinct(track_id),\n    unique_artists = n_distinct(artist_id)\n  ) |&gt;\n  knitr::kable(caption = \"Number of Unique Tracks and Artists\")\n\n\n\nNumber of Unique Tracks and Artists\n\n\nunique_tracks\nunique_artists\n\n\n\n\n57884\n14973\n\n\n\n\n\nCode\ntop_5_tracks &lt;- rectangular_df |&gt;\n  count(track_id, track_name, artist_name, sort = TRUE) |&gt;\n  slice_head(n = 5)\n\ntop_5_tracks |&gt;\n  knitr::kable(caption = \"Top 5 Most Frequently Appearing Tracks in Playlists\")\n\n\n\nTop 5 Most Frequently Appearing Tracks in Playlists\n\n\n\n\n\n\n\n\ntrack_id\ntrack_name\nartist_name\nn\n\n\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\nDrake\n98\n\n\n7KXjTSCq5nL1LoYtL7XAwS\nHUMBLE.\nKendrick Lamar\n96\n\n\n7yyRTcZmCiyzzJlNzGC9Ol\nBroccoli (feat. Lil Yachty)\nDRAM\n87\n\n\n3a1lNhkSLSkpJE4MSHpDu9\nCongratulations\nPost Malone\n86\n\n\n4Km5HrUvYTaSUfiSGPJeQR\nBad and Boujee (feat. Lil Uzi Vert)\nMigos\n84\n\n\n\n\n\nCode\nSONGS &lt;- SONGS |&gt; \n  rename(track_id = id)\n\nmissing_in_songs &lt;- rectangular_df |&gt;\n  dplyr::anti_join(SONGS, by = \"track_id\") |&gt;\n  dplyr::count(track_name, sort = TRUE) |&gt;\n  dplyr::slice_head(n = 1)\n\nmissing_in_songs |&gt;\n  knitr::kable(caption = \"Most Frequently Appearing Track Missing from SONGS Data\")\n\n\n\nMost Frequently Appearing Track Missing from SONGS Data\n\n\ntrack_name\nn\n\n\n\n\nOne Dance\n98\n\n\n\n\n\nCode\nmost_danceable &lt;- SONGS |&gt;\n  dplyr::arrange(desc(danceability)) |&gt;\n  dplyr::slice(1)\n\nmost_danceable |&gt;\n  select(name, artist, danceability, popularity, year) |&gt;\n  knitr::kable(caption = \"Most Danceable Track in the SONGS Dataset\")\n\n\n\nMost Danceable Track in the SONGS Dataset\n\n\nname\nartist\ndanceability\npopularity\nyear\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\n57\n1989\n\n\n\n\n\nCode\nappearance_count &lt;- rectangular_df |&gt;\n  dplyr::filter(track_id == most_danceable$track_id) |&gt;\n  dplyr::count() |&gt;\n  dplyr::rename(`Playlist Appearances` = n)\n\nappearance_count |&gt;\n  knitr::kable(caption = \"How Often the Most Danceable Track Appears in Playlists\")\n\n\n\nHow Often the Most Danceable Track Appears in Playlists\n\n\nPlaylist Appearances\n\n\n\n\n1\n\n\n\n\n\nCode\nlongest_avg_playlist &lt;- rectangular_df |&gt;\n  dplyr::group_by(playlist_name, playlist_id) |&gt;\n  dplyr::summarize(avg_duration = mean(duration, na.rm = TRUE), .groups = \"drop\") |&gt;\n  dplyr::arrange(desc(avg_duration)) |&gt;\n  dplyr::slice(1)\n\nlongest_avg_playlist |&gt;\n  dplyr::mutate(avg_duration = round(avg_duration / 60000, 2)) |&gt;\n  dplyr::rename(`Average Track Duration (min)` = avg_duration) |&gt;\n  knitr::kable(caption = \"Playlist with the Longest Average Track Duration\")\n\n\n\nPlaylist with the Longest Average Track Duration\n\n\nplaylist_name\nplaylist_id\nAverage Track Duration (min)\n\n\n\n\nLust\n1790\n9.18\n\n\n\n\n\nCode\nmost_followed &lt;- rectangular_df |&gt;\n  dplyr::distinct(playlist_id, playlist_name, playlist_followers) |&gt;\n  dplyr::arrange(desc(playlist_followers)) |&gt;\n  dplyr::slice(1)\n\nmost_followed |&gt;\n  knitr::kable(caption = \"Most Followed Playlist in the Dataset\")\n\n\n\nMost Followed Playlist in the Dataset\n\n\nplaylist_id\nplaylist_name\nplaylist_followers\n\n\n\n\n765\nTangled\n1038\n\n\n\n\n\nCode\nSONGS_unique &lt;- SONGS |&gt; \n  distinct(track_id, .keep_all = TRUE)\ncombined_df &lt;- inner_join(rectangular_df, SONGS_unique, by = \"track_id\")"
  },
  {
    "objectID": "mp03.html#analysis-with-visualizations",
    "href": "mp03.html#analysis-with-visualizations",
    "title": "The Loverboy Playlist",
    "section": "Analysis with Visualizations",
    "text": "Analysis with Visualizations\n\n\nCode\ntrack_popularity &lt;- combined_df |&gt;\n  group_by(track_id, track_name, artist_name) |&gt;\n  summarise(\n    playlist_appearances = n(),\n    avg_popularity = mean(popularity, na.rm = TRUE),\n    .groups = \"drop\"\n  )\nggplot(track_popularity, aes(x = playlist_appearances, y = avg_popularity)) +\n  geom_point(alpha = 0.4, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_log10() +  # log scale to handle skewed data\n  labs(\n    title = \"Popularity vs Playlist Appearances\",\n    x = \"Playlist Appearances (log scale)\",\n    y = \"Spotify Popularity Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the graph, we can interpret that as a song gets more popular, it will appear on playlists more.\n\n\nCode\nggplot(combined_df, aes(x = popularity)) +\n  geom_histogram(binwidth = 5, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Spotify Popularity Scores\",\n    x = \"Popularity Score\",\n    y = \"Number of Tracks\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe popularity scores of the songs from the dataset seem to be a majority around the 50-75 scores.\n\n\nCode\npopular_threshold &lt;- 65\npopular_songs &lt;- combined_df |&gt; filter(popularity &gt;= popular_threshold)\npopular_songs |&gt;\n  count(year) |&gt;\n  ggplot(aes(x = year, y = n)) +\n  geom_col(fill = \"forestgreen\") +\n  labs(\n    title = \"Most Popular Songs by Release Year\",\n    x = \"Release Year\",\n    y = \"Number of Popular Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the graph, we can tell that more songs are getting popular in recent times than in the past, where it was rarer for a song to become popular.\n\n\nCode\ntop_dance_year &lt;- popular_songs |&gt;\n  group_by(year) |&gt;\n  summarise(avg_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_danceability)) |&gt;\n  slice(1)\ntop_dance_year\n\n\n# A tibble: 1 √ó 2\n   year avg_danceability\n  &lt;dbl&gt;            &lt;dbl&gt;\n1  1982            0.741\n\n\nCode\npopular_songs |&gt;\n  group_by(year) |&gt;\n  summarise(avg_danceability = mean(danceability, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = avg_danceability)) +\n  geom_line(color = \"tomato\", size = 1.2) +\n  labs(\n    title = \"Average Danceability by Year (Popular Songs)\",\n    x = \"Release Year\",\n    y = \"Danceability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the graph, we can see that the average danceabiliy of each year changes drastically every year, with the peak year of average danceability being 1982.\n\n\nCode\ncombined_df |&gt;\n  filter(!is.na(year)) |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  count(decade) |&gt;\n  ggplot(aes(x = factor(decade), y = n)) +\n  geom_col(fill = \"darkorchid\") +\n  labs(\n    title = \"Most Represented Decade in User Playlists\",\n    x = \"Decade\",\n    y = \"Number of Tracks\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe are able to see that the most represented decade in users playlists are in the 2010s to 2020, with users clearly not leaning towards adding older songs into their playlists.\n\n\nCode\nkey_labels &lt;- c(\"C\", \"C‚ôØ/D‚ô≠\", \"D\", \"D‚ôØ/E‚ô≠\", \"E\", \"F\", \n                \"F‚ôØ/G‚ô≠\", \"G\", \"G‚ôØ/A‚ô≠\", \"A\", \"A‚ôØ/B‚ô≠\", \"B\")\ncombined_df |&gt;\n  filter(!is.na(key)) |&gt;\n  count(key) |&gt;\n  mutate(key_label = factor(key_labels[key + 1], levels = key_labels)) |&gt;\n  ggplot(aes(x = key_label, y = n, fill = key_label)) +\n  geom_col(show.legend = FALSE) +\n  coord_polar() +\n  labs(\n    title = \"Key Frequency Among Songs\",\n    x = NULL, y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSince every song has different keys, I decided to look at all of them and we are able to see that C‚ôØ/D‚ô≠ is the most common key in songs.\n\n\nCode\ncombined_df |&gt;\n  mutate(duration_min = duration / 60000) |&gt;\n  ggplot(aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.25, fill = \"steelblue\", color = \"white\") +\n  coord_cartesian(xlim = c(0, 10)) +\n  labs(\n    title = \"Distribution of Track Lengths\",\n    x = \"Duration (minutes)\",\n    y = \"Number of Tracks\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the graph, we are able to see that the graph is bell shaped, with majority of songs being between 2.5-5 minutes long.\n\n\nCode\ncombined_df |&gt;\n  mutate(explicit = ifelse(explicit == 1, \"Explicit\", \"Clean\")) |&gt;\n  ggplot(aes(x = explicit, y = popularity, fill = explicit)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    title = \"Are Explicit Songs More Popular?\",\n    x = \"Song Type\",\n    y = \"Spotify Popularity\"\n  ) +\n  scale_fill_manual(values = c(\"steelblue\", \"tomato\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFor my first exploratory question, I was curious if explicit songs have an effect on how popular a song is, and from the graph we are able to see that explicit songs are slightly more popular than clean songs.\n\n\nCode\nexplicit_by_decade &lt;- combined_df |&gt;\n  filter(!is.na(year)) |&gt;\n  mutate(\n    decade = (year %/% 10) * 10,\n    explicit = ifelse(explicit == 1, \"Explicit\", \"Clean\")\n  ) |&gt;\n  count(decade, explicit)\nggplot(explicit_by_decade, aes(x = factor(decade), y = n, fill = explicit)) +\n  geom_col(position = \"fill\") +  # Use \"fill\" for proportions, \"stack\" for raw counts\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"Clean\" = \"steelblue\", \"Explicit\" = \"firebrick\")) +\n  labs(\n    title = \"Proportion of Explicit Songs by Decade\",\n    x = \"Decade\",\n    y = \"Proportion of Songs\",\n    fill = \"Track Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nI wanted to see the difference between the amount of songs from each decade that were explicit or clean, with explicit songs only becoming poopular around the 90‚Äôs and now almost half the songs on playlists are explicit, showing how artists are evolving over time."
  },
  {
    "objectID": "mp03.html#building-the-loverboy-playlist",
    "href": "mp03.html#building-the-loverboy-playlist",
    "title": "The Loverboy Playlist",
    "section": "Building The Loverboy Playlist",
    "text": "Building The Loverboy Playlist\nWith the anchor song being The Way Life Goes, from the combined dataset I was able to gather data on the songs in the same playlist, songs in the same key and tempo, songs from the same artist, and songs from the same year with similar audio. From there, I was able to look through each dataset and choose songs from each of them, giving me a diverse yet similar array of songs to choose to make my ultimate playlist.\n\n\nCode\nanchor_tracks &lt;- combined_df |&gt; \n  filter(track_name == \"The Way Life Goes (feat. Oh Wonder)\", \n         artist_name == \"Lil Uzi Vert\")\nsame_playlist_songs &lt;- combined_df |&gt;\n  filter(playlist_id %in% anchor_tracks$playlist_id) |&gt;\n  filter(track_id != anchor_tracks$track_id[1]) |&gt;\n  distinct(track_id, track_name, artist_name, .keep_all = TRUE)\n\nsame_playlist_songs |&gt;\n  select(track_name, artist_name, playlist_name) |&gt;\n  datatable(\n    caption = \"Songs from the Same Playlist as the Anchor Track\",\n    options = list(pageLength = 10, scrollX = TRUE)\n  )\n\n\n\n\n\n\nCode\nsame_key_tempo &lt;- combined_df |&gt;\n  filter(key == anchor_tracks$key[1]) |&gt;\n  filter(abs(tempo - anchor_tracks$tempo[1]) &lt;= 5) |&gt;\n  filter(track_id != anchor_tracks$track_id[1]) |&gt;\n  distinct(track_id, track_name, artist_name, .keep_all = TRUE)\n\nsame_key_tempo |&gt;\n  select(track_name, artist_name, key, tempo) |&gt;\n  datatable(\n    caption = \"Songs with the Same Key and Similar Tempo\",\n    options = list(pageLength = 10, scrollX = TRUE)\n  )\n\n\n\n\n\n\nCode\nsame_artist &lt;- combined_df |&gt;\n  filter(artist_name == anchor_tracks$artist_name[1]) |&gt;\n  filter(track_id != anchor_tracks$track_id[1]) |&gt;\n  distinct(track_id, track_name, .keep_all = TRUE)\n\nsame_artist |&gt;\n  select(track_name, artist_name, album_name, year, popularity) |&gt;\n  datatable(\n    caption = \"Songs by the Same Artist as the Anchor Track\",\n    options = list(pageLength = 10, scrollX = TRUE)\n  )\n\n\n\n\n\n\nCode\nsame_year_similar_audio &lt;- combined_df |&gt;\n  filter(year == anchor_tracks$year[1]) |&gt;\n  filter(\n    abs(danceability - anchor_tracks$danceability[1]) &lt;= 0.1,\n    abs(energy - anchor_tracks$energy[1]) &lt;= 0.1,\n    abs(acousticness - anchor_tracks$acousticness[1]) &lt;= 0.1\n  ) |&gt;\n  filter(track_id != anchor_tracks$track_id[1]) |&gt;\n  distinct(track_id, track_name, artist_name, .keep_all = TRUE)\n\nsame_year_similar_audio |&gt;\n  select(track_name, artist_name, year, danceability, energy, acousticness) |&gt;\n  datatable(\n    caption = \"Songs from the Same Year with Similar Audio Features\",\n    options = list(pageLength = 10, scrollX = TRUE)\n  )"
  },
  {
    "objectID": "mp03.html#the-ultimate-playlist",
    "href": "mp03.html#the-ultimate-playlist",
    "title": "The Loverboy Playlist",
    "section": "The Ultimate Playlist",
    "text": "The Ultimate Playlist\nAfter looking through all the songs with similar artists, playlists, tempos and more, I finally created my ultimate playlist and analyzed the energy, danceability, and valence of the tracks with a focus on the energy because I wanted it to progress throughout the playlist, gradually building emotion and in the end end making the listener feel empowered.\n\n\nCode\nall_candidates &lt;- bind_rows(\n  same_playlist_songs,\n  same_key_tempo,\n  same_artist,\n  same_year_similar_audio,\n  anchor_tracks\n) |&gt; \n  distinct(track_id, .keep_all = TRUE)\nfinal_playlist &lt;- all_candidates |&gt; \n  filter(track_name %in% c(\n    \"The Way Life Goes (feat. Oh Wonder)\",\n    \"90210\",\n    \"Â§úÊõ≤\",\n    \"Runaway\",\n    \"Fallingforyou\",\n    \"Girls\",\n    \"It Takes Time\",\n    \"Hopeless\",\n    \"Love Galore\",\n    \"Feel No Ways\",\n    \"Nights\",\n    \"LOVE. FEAT. ZACARI.\"\n  ))\nfinal_playlist &lt;- final_playlist |&gt;\n  arrange(energy) |&gt;\n  mutate(\n    order = row_number(),\n    track_name = factor(track_name, levels = track_name)\n  )\nfinal_playlist |&gt;\n  select(track_name, artist_name, year, popularity, energy, danceability) |&gt;\n  knitr::kable(caption = \"Final Playlist: The Loverboy Collection\")\n\n\n\nFinal Playlist: The Loverboy Collection\n\n\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\nyear\npopularity\nenergy\ndanceability\n\n\n\n\nFallingforyou\nThe 1975\n2013\n61\n0.285\n0.264\n\n\nIt Takes Time\nTrippie Redd\n2017\n69\n0.304\n0.845\n\n\n90210\nTravis Scott\n2015\n75\n0.526\n0.402\n\n\nNights\nFrank Ocean\n2016\n78\n0.548\n0.466\n\n\nRunaway\nKanye West\n2010\n72\n0.568\n0.571\n\n\nLOVE. FEAT. ZACARI.\nKendrick Lamar\n2017\n79\n0.585\n0.800\n\n\nLove Galore\nSZA\n2017\n76\n0.594\n0.795\n\n\nÂ§úÊõ≤\nJay Chou\n2005\n55\n0.649\n0.674\n\n\nFeel No Ways\nDrake\n2016\n70\n0.674\n0.588\n\n\nGirls\nLil Peep\n2017\n68\n0.697\n0.644\n\n\nHopeless\nKhalid\n2017\n62\n0.702\n0.785\n\n\nThe Way Life Goes (feat. Oh Wonder)\nLil Uzi Vert\n2017\n78\n0.757\n0.703\n\n\n\n\n\nCode\nggplot(final_playlist, aes(x = track_name)) +\n  geom_line(aes(y = energy), color = \"orange\", size = 1.2, group = 1) +\n  geom_line(aes(y = danceability), color = \"steelblue\", size = 1.2, group = 1) +\n  geom_line(aes(y = valence), color = \"purple\", size = 1.2, group = 1) +\n  labs(\n    title = \"Ultimate Playlist: The Loverboy Collection\",\n    subtitle = \"Energy (orange), Danceability (blue), Valence (purple)\",\n    x = \"Track Name\",\n    y = \"Feature Value (0‚Äì1 scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n    plot.title = element_text(face = \"bold\")\n  )"
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "The Loverboy Playlist",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChing-Wei Chen, Paul Lamere, Markus Schedl, and Hamed Zamani. Recsys Challenge 2018: Automatic Music Playlist Continuation. In Proceedings of the 12th ACM Conference on Recommender Systems (RecSys ‚Äô18), 2018.‚Ü©Ô∏é"
  }
]